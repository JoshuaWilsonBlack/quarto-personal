[
  {
    "objectID": "dociteproc/example.html",
    "href": "dociteproc/example.html",
    "title": "Dociteproc Example",
    "section": "",
    "text": "This filter adds formatting to heading text."
  },
  {
    "objectID": "dociteproc/example.html#heading",
    "href": "dociteproc/example.html#heading",
    "title": "Dociteproc Example",
    "section": "",
    "text": "This filter adds formatting to heading text."
  },
  {
    "objectID": "experience.html",
    "href": "experience.html",
    "title": "Joshua Wilson Black",
    "section": "",
    "text": "2024 - | Lecturer | University of Canterbury\n2021 - 2024 | Post Doctoral Fellow | University of Canterbury\n2014 - 2018 | Graduate Teaching Assistant | University of Sheffield"
  },
  {
    "objectID": "education.html",
    "href": "education.html",
    "title": "Joshua Wilson Black",
    "section": "",
    "text": "2021 | Master of Applied Data Science (with Distinction) | University of Canterbury\n2017 | Doctor of Philosophy | University of Sheffield\n2013 | Master of Arts | University of Waikato\n2012 | Bachelor of Science with Honours (First Class) | University of Canterbury\n2011 | Bachelor of Science | University of Canterbury\n2011 | Bachelor of Arts | University of Canterbury"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Wilson Black, Joshua, and James Brand. 2024. “nzilbb.vowels: Vowel Covariation Tools.” https://nzilbb.github.io/nzilbb_vowels/.\n\n\nMaclagan, Margaret, Toby Macrae, and Joshua Wilson Black. 2024. “The Pronunciation of the Word ‘Māori’.” Te Reo: The Journal of the Linguistic Society of New Zealand 66 (2): 132–50. https://nzlingsoc.org/journal_article/the-pronunciation-of-the-word-maori/.\n\n\nWilson Black, Joshua, Jennifer Hay, Lynn Clark, and James Brand. 2023. “The Overlooked Effect of Amplitude on Within-Speaker Vowel Variation.” Linguistics Vanguard 9 (1). Walter de Gruyter GmbH: 173–89. doi:10.1515/lingvan-2022-0086.\n\n\nFromont, Robert, Lynn Clark, Joshua Wilson Black, and Margaret Blackwood. 2023. “Maximizing Accuracy of Forced Alignment for Spontaneous Child Speech.” Language Development Research 3 (1). doi:10.34842/shrr-sv10.\n\n\nWilson Black, Joshua. 2023. “Peirce on Metaphysics and Common Sense Belief: A Challenge to Hookway’s Account.” In Pragmatic Reason: Christopher Hookway and the American Philosophical Tradition, edited by Robert Talisse, Paniel Reyes Cárdenas, and Daniel Herbert. Routledge. doi:10.4324/9781003165699.\n\n\n———. 2022. “Creating Specialized Corpora from Digitized Historical Newspaper Archives.” Digital Scholarship in the Humanities 38 (2). Oxford University Press (OUP): 779–97. doi:10.1093/llc/fqac079.\n\n\nWilson Black, Joshua, James Brand, Jen Hay, and Lynn Clark. 2022. “Using Principal Component Analysis to Explore Co‐variation of Vowels.” Language and Linguistics Compass 17 (1). Wiley. doi:10.1111/lnc3.12479.\n\n\nLegg, Catherine, and Joshua Black. 2022. “What Is Intelligence For? A Peircean Pragmatist Response to the Knowing-How, Knowing-That Debate.” Erkenntnis: An International Journal of Scientific Philosophy 87 (5). Dordrecht: Springer Netherlands: 2265–84. doi:10.1007/s10670-020-00301-9.\n\n\nBlack, Joshua David. 2017. “Peirce’s Conception of Metaphysics.” University of Sheffield. https://etheses.whiterose.ac.uk/18188/.\n\n\nBlack, Joshua. 2016. “Peirce.” British Journal for the History of Philosophy 24 (6). Routledge: 1223–27. doi:10.1080/09608788.2016.1149448.\n\n\nBlack, Joshua David. 2013. “Peirce on Habit, Practice, and Theory: The Priority of Practice and the Autonomy of Theory.” https://researchcommons.waikato.ac.nz/handle/10289/7847."
  },
  {
    "objectID": "url2slides/example.html",
    "href": "url2slides/example.html",
    "title": "Url2slides Example",
    "section": "",
    "text": "This filter adds formatting to heading text."
  },
  {
    "objectID": "url2slides/example.html#heading",
    "href": "url2slides/example.html#heading",
    "title": "Url2slides Example",
    "section": "",
    "text": "This filter adds formatting to heading text."
  },
  {
    "objectID": "posts/digital-scholarship-in-the-humanities/index.html",
    "href": "posts/digital-scholarship-in-the-humanities/index.html",
    "title": "New Publication: Creating Specialised Corpora from Digitized Historical Newspaper Archives",
    "section": "",
    "text": "One of the promises of digital humanities for the ‘historical sciences’ is that we’ll be able to, in Tim Hitchcock’s words, shift from ‘piles of books’ to ‘maps of meaning’. That is, we’ll be able to produce high level representations of phenomena across large collections of text. These could come in all sorts of forms. But, for a concrete example, consider the following:\n\nThis is a ‘cooccurence network’ for the term ‘materialism’ as it appears in a corpus of pre-1900 newspaper items from New Zealand. It depicts terms which a connected to ‘materialism’ in a statistically surprising way and terms which are similarly surprisingly connected to those terms. Networks of this sort can both motivate specific research questions and provide a depiction of the wider context for the use of a given term. More such networks can be produced here. The same link provides an explanation of the meaning of these connections.\nWhen confronted with a huge collections of digitised material, it is very hard for an individual researcher or small team of researchers with interests in a specialised topic and limited computational researchers to apply contemporary text mining and other computational methods. With 1000s of GPUs and 1000s of hours to run them (not to mention the gas to burn), you can create a highly general representation of what is going on in a collection of texts and then query it about specific topics. This is one way of thinking about what ChatGPT etc are: interactive general representations of phenomena in this or that large-scale collection of texts (rather than ‘intelligence’ in any sense I’d consider meaningful). We thus need methods for selecting items for inclusion in corpora. Moreover, for reasons described in the paper, we want methods which do not rely on simple keyword searches.\nMy contribution to this problem has just been published with open access in Digital Scholarship in the Humanities. The paper is called “Creating specialized corpora from digitized historical newspaper archives: An iterative bootstrapping approach”. It provides a methodological motivation for applying text mining techniques to historical datasets and sets up the need for methods to select subsets of these datasets for analysis. It then provides a general approach to producing corpora for analysis from large historical newspaper collections along with Python code for each stage of the process.\nThe core idea of the method is that the text mining methods which we want to apply to gain insight into a specialised topic can also be used to generate increasingly focused corpora. We start with some initial candidate corpus, and apply methods such as cooccurrence network analysis or topic modelling, in order to find material which is either relevant or not relevant to our topic. We label the material and then train naive Bayes classifiers, which are then applied to the full newspaper dataset to generate a new candidate corpus. That is, we work upwards from an inappropriate corpus of items (inappropriate by including lots of irrelevant material and/or excluding relevant material). As we iterate this process, the corpus becomes increasingly targeted. I illustrate the method by generating a corpus of philosophical discourse in pre-1900 English-language New Zealand newspaper data made available by the National Library of New Zealand.\nI’m a bit slow in making a noise about this publication. It came out as I was in the hospital for the birth of my first child, Clement. Clem and his mum are doing very well. I am now returning to work now after my parental leave.\nI should also say that in the proofing phase I missed the fact that my sectioning was flattened to a single level! Perhaps this was (pre-)baby brain.\nWhat are the next steps? This paper came out of a summer project towards my data science qualification. As such, it very much focuses on the general methodological problem outlined above. I am currently working on a paper in the history of philosophy which actually uses the corpus I report on in this paper. The basic idea is to focus attention use the newspaper controversy generated by William Salmond’s pamphlet The Reign of Grace (see Wood, 2014). Salmond argues, against then overwhelmingly dominant forms of Christianity, that human beings can be saved after they die. A combination of traditional close reading and the use of cooccurrence networks will be used to reveal deep connections between the theological questions of heaven and hell and the live political questions about nature and purpose of punishment. But this is another story.\n\n\n\nAn ancestor of mine in the Papers Past dataset."
  },
  {
    "objectID": "posts/visualising-vowel-space-gamms/index.html",
    "href": "posts/visualising-vowel-space-gamms/index.html",
    "title": "Visualising Vowel Space Change with GAMs",
    "section": "",
    "text": "Multiple recent projects at NZILBB have used Generalised Mixed Models (GAMMs) to investigate changes in vowel spaces both across multiples speakers and within single speakers.\nIn such projects, it is useful to visualise changes to vowel spaces over time with both static plots and animations.\nThis post sets out a structure for fitting models of the first and second formants of a series of vowels and for visualising them together within vowel space diagrams.\nThis general structure, and some specific code for visualisation, was originally developed by James Brand for Brand et al. 2021.\nI’ll assume the reader knows something about vowels and vowel spaces, the basics of data manipulation with dplyr, and setting up models in R.\n\n\n\nThe New Zealand Broadcasting Service Mobile Unit (source)"
  },
  {
    "objectID": "posts/visualising-vowel-space-gamms/index.html#introduction",
    "href": "posts/visualising-vowel-space-gamms/index.html#introduction",
    "title": "Visualising Vowel Space Change with GAMs",
    "section": "",
    "text": "Multiple recent projects at NZILBB have used Generalised Mixed Models (GAMMs) to investigate changes in vowel spaces both across multiples speakers and within single speakers.\nIn such projects, it is useful to visualise changes to vowel spaces over time with both static plots and animations.\nThis post sets out a structure for fitting models of the first and second formants of a series of vowels and for visualising them together within vowel space diagrams.\nThis general structure, and some specific code for visualisation, was originally developed by James Brand for Brand et al. 2021.\nI’ll assume the reader knows something about vowels and vowel spaces, the basics of data manipulation with dplyr, and setting up models in R.\n\n\n\nThe New Zealand Broadcasting Service Mobile Unit (source)"
  },
  {
    "objectID": "posts/visualising-vowel-space-gamms/index.html#fitting-multiple-models-with-purrr-and-mgcv",
    "href": "posts/visualising-vowel-space-gamms/index.html#fitting-multiple-models-with-purrr-and-mgcv",
    "title": "Visualising Vowel Space Change with GAMs",
    "section": "Fitting Multiple Models with purrr and mgcv",
    "text": "Fitting Multiple Models with purrr and mgcv\n\nSetup\nWe’re going to fit these models with a small subset of the data from the Origins of New Zealand English (ONZE) corpus. This dataset contains first and second formant data for 100 speakers of New Zealand English (for details see supplementaries for Brand et al. 2021). The data can be found here.\nFor the purposes of this post any similar data set would be fine. We need:\n\nfirst and second formant data,\na range of vowels (we’ll only look at monophthongs here),\na time variables (whether year of birth, age category, or time through recording), and\nany variables you wish to control for.\n\nLet’s load the libraries we will use and have a look at the data.\n\n# Load tidyverse and friends.\nlibrary(tidyverse)\nlibrary(gganimate)\n\n# mgcv will be used for fitting gamms later and itsadug for visualisation\nlibrary(mgcv)\nlibrary(itsadug)\n\n# kable for displaying the dataset.\nlibrary(kableExtra)\n\nvowels &lt;- read_rds('anon_ONZE_mean_sample.rds')\n\nvowels %&gt;%\n  head(10) %&gt;%\n  kable() %&gt;%\n  kable_styling(font_size = 11) %&gt;%\n  scroll_box(width = \"100%\")\n\n\n\n\n\nSpeaker\nVowel\nF1_50\nF2_50\nSpeech_rate\nGender\nyob\n\n\n\n\nCC_f_020\nDRESS\n622.5797\n1897.986\n4.9264\nF\n1936\n\n\nCC_f_020\nFLEECE\n518.7458\n1882.763\n4.9264\nF\n1936\n\n\nCC_f_020\nGOOSE\n524.0606\n1944.152\n4.9264\nF\n1936\n\n\nCC_f_020\nKIT\n646.8214\n1790.054\n4.9264\nF\n1936\n\n\nCC_f_020\nLOT\n735.3438\n1262.969\n4.9264\nF\n1936\n\n\nCC_f_020\nNURSE\n534.8571\n1912.714\n4.9264\nF\n1936\n\n\nCC_f_020\nSTART\n888.3929\n1604.750\n4.9264\nF\n1936\n\n\nCC_f_020\nSTRUT\n831.9661\n1511.373\n4.9264\nF\n1936\n\n\nCC_f_020\nTHOUGHT\n594.5000\n1051.571\n4.9264\nF\n1936\n\n\nCC_f_020\nTRAP\n637.8667\n1816.833\n4.9264\nF\n1936\n\n\n\n\n\n\n\nIn this dataset each row is a vowel token, with columns:\n\nF1_50 and F2_50: F1 and F2, taken at the midpoint measured in Hz,\nVowel: Wells lexical set labels for New Zealand English monophthongs,\nyob: participant year of birth (our time variable),\nSpeech_rate: the average speech rate of the participant across the recording (a control variable),\nSpeaker: a code indicating which speaker the token comes from (sometimes useful as a random effect), and\nGender: the gender of the speaker (in this case, an M/F binary).\n\nIn any real research project, you will need to engage in a lot of data exploration here. Do you have good data coverage? Is there evidence of outliers in the data? Does the data need to be normalised? This is the time to ask this kind of question. The answers will, of course, depend on your research questions. For this post, the only point of this data is to illustrate a method for modelling and visualising. We can skip these questions!\nWe will now fit separate models for the F1 and F2 of each vowel. Rather than using a big for loop, or fitting each model with a separate line of code, we will use the purrr method of nesting our data so that we have a row for each of the models we want to fit, fit the models, and then unnest to produce data which can be used to visualise our model. We nest, we mutate, and we unnest. This is a common pattern with purrr.\nBefore we nest, we need to slightly modify our data. Rather than having a column for our F1 data and a column for our F2 data, we want to capture there in rows. That is, we need our table to be longer. There will then be two rows for each token, one for the F1 and one for the F2.\nTo do this, we use the trusty dplyr function pivot_longer():\n\nvowels &lt;- vowels %&gt;%\n  pivot_longer(\n    cols = F1_50:F2_50, # Select the columns to turn into rows.\n    names_to = \"Formant\", # Name the column to indicate if data is F1 or F2,\n    values_to = \"Frequency\"\n  )\n\nvowels %&gt;%\n  head(10) %&gt;%\n  kable() %&gt;%\n  kable_styling(font_size = 11) %&gt;%\n  scroll_box(width = \"100%\")\n\n\n\n\n\nSpeaker\nVowel\nSpeech_rate\nGender\nyob\nFormant\nFrequency\n\n\n\n\nCC_f_020\nDRESS\n4.9264\nF\n1936\nF1_50\n622.5797\n\n\nCC_f_020\nDRESS\n4.9264\nF\n1936\nF2_50\n1897.9855\n\n\nCC_f_020\nFLEECE\n4.9264\nF\n1936\nF1_50\n518.7458\n\n\nCC_f_020\nFLEECE\n4.9264\nF\n1936\nF2_50\n1882.7627\n\n\nCC_f_020\nGOOSE\n4.9264\nF\n1936\nF1_50\n524.0606\n\n\nCC_f_020\nGOOSE\n4.9264\nF\n1936\nF2_50\n1944.1515\n\n\nCC_f_020\nKIT\n4.9264\nF\n1936\nF1_50\n646.8214\n\n\nCC_f_020\nKIT\n4.9264\nF\n1936\nF2_50\n1790.0536\n\n\nCC_f_020\nLOT\n4.9264\nF\n1936\nF1_50\n735.3438\n\n\nCC_f_020\nLOT\n4.9264\nF\n1936\nF2_50\n1262.9688\n\n\n\n\n\n\n\nAs the table above shows, we now have a column indicating whether a frequency value is an F1 or an F2 reading.\n\n\nNest\nWe now nest the data. We do this by grouping the data by the columns which identify the models we want to fit. In this case, we fit an F1 and an F2 model for each vowel. So the columns we need to identify our models are Vowel and Formant. Once we’ve grouped, we simply use the function nest().\n\nvowels &lt;- vowels %&gt;%\n  group_by(Vowel, Formant) %&gt;%\n  nest()\n  \nvowels\n\n# A tibble: 20 × 3\n# Groups:   Vowel, Formant [20]\n   Vowel   Formant data              \n   &lt;fct&gt;   &lt;chr&gt;   &lt;list&gt;            \n 1 DRESS   F1_50   &lt;tibble [100 × 5]&gt;\n 2 DRESS   F2_50   &lt;tibble [100 × 5]&gt;\n 3 FLEECE  F1_50   &lt;tibble [100 × 5]&gt;\n 4 FLEECE  F2_50   &lt;tibble [100 × 5]&gt;\n 5 GOOSE   F1_50   &lt;tibble [100 × 5]&gt;\n 6 GOOSE   F2_50   &lt;tibble [100 × 5]&gt;\n 7 KIT     F1_50   &lt;tibble [100 × 5]&gt;\n 8 KIT     F2_50   &lt;tibble [100 × 5]&gt;\n 9 LOT     F1_50   &lt;tibble [100 × 5]&gt;\n10 LOT     F2_50   &lt;tibble [100 × 5]&gt;\n11 NURSE   F1_50   &lt;tibble [100 × 5]&gt;\n12 NURSE   F2_50   &lt;tibble [100 × 5]&gt;\n13 START   F1_50   &lt;tibble [100 × 5]&gt;\n14 START   F2_50   &lt;tibble [100 × 5]&gt;\n15 STRUT   F1_50   &lt;tibble [100 × 5]&gt;\n16 STRUT   F2_50   &lt;tibble [100 × 5]&gt;\n17 THOUGHT F1_50   &lt;tibble [100 × 5]&gt;\n18 THOUGHT F2_50   &lt;tibble [100 × 5]&gt;\n19 TRAP    F1_50   &lt;tibble [100 × 5]&gt;\n20 TRAP    F2_50   &lt;tibble [100 × 5]&gt;\n\n\nThe output above shows that we now have a three column data frame (or, in tidyverse speak, a tibble), with the familiar columns Vowel and Formant and a new column data. The column data contains tibbles which live inside our tibble. That is, nested tibbles. These contain the data which we will use to model each vowel and formant separately.\n\n\nMutate (and Map)\nWe can perform actions on the tibbles in the data column by using mutate (just as we would modify other data in a tibble). In this case we will create a column to store our models. The basic structure will look something like this.\n\nvowels &lt;- vowels %&gt;%\n  mutate(\n    model = #???\n  )\n\nThe question marks can be filled in with the map() function. This enables us to apply a function to fit a model to each of our nested tibbles.\nSo, what will this function look like? This is not the place for a tutorial on GAMs (for which, go here). We will fit a model which predicts formant frequency from the gender of each participant, their year of birth, and their average speech rate. One way to implement this structure in mgcv is with the formula Frequency ~ Gender + s(yob, by=Gender) + s(Speech_rate).\nThis structure will be the same for all of our nested tibbles. The only thing that will change is the data fed in to it. In this kind of case, we can use ~ to turn our model expression into a function and .x as a pronoun for the nested tibbles. Let’s see what this look like and then explain further:\n\nvowels &lt;- vowels %&gt;%\n  mutate(\n    model = map(\n      data, # We are applying a function to the entries of the `data` column.\n      # This is the function we are applying (introduced with a ~)\n      ~ bam( \n          # Here's our formula.\n          Frequency ~ Gender + \n            s(yob, by=Gender, k=5) + \n            s(Speech_rate, k=5),\n          data = .x, # Here's our pronoun.\n          # Then some arguments to speed up the model fit.\n          method = 'fREML',\n          discrete = TRUE,\n          nthreads = 2\n      )\n    )\n  )\n\nThe function bam is one of the main functions for fitting GAMM models in mgcv. It is often used for large data sets. Our use of ~ creates a function which is applied to each of our nested tibbles. The entries in model are created by taking the corresponding tibble in the data column and applying the function to it. The tibble is referred to by the pronoun .x in the function. In the above, .x is used as the data fed to bam(). So, for the row of our tibble with DRESS and F1_50 in the Vowel and Formant columns, the entry for model will be a model produced from the data for dress and F1.\nNote that you can use any modelling function you like here. The general strategy of nesting and fitting models doesn’t have any special connections with GAMs or mgcv. You could fit a linear model with lm or a generalised linear model using the lme4 package.\nWe now have GAMM models for each of these vowels. These are stored in the model column we have just created. We can check out one of these models using the itsadug plot_smooth() function as follows:\n\nplot_smooth(\n  vowels$model[[7]], # Pick the first entry in the model column.\n  view = \"yob\", # The x-axis variable.  \n  plot_all = \"Gender\",\n  main= paste(vowels$Vowel[[7]], vowels$Formant[[7]])\n)\n\nSummary:\n    * Gender : factor; set to the value(s): F, M. \n    * yob : numeric predictor; with 30 values ranging from 1864.000000 to 1981.000000. \n    * Speech_rate : numeric predictor; set to the value(s): 4.5118. \n    * NOTE : No random effects in the model to cancel.\n \n\n\n\n\n\nSmooth plot for KIT F1\n\n\n\n\nFigure 1 shows a well known feature of the development of New Zealand English: the centralisation of the kit vowel. It also indicates something to keep in mind when fitting non-linear models. The wiggles in the smooth for the female speakers might simply be over fitting our particular sample. Again, this is illustrative of a general pattern, each step of which requires criticism in practice!\nFor the purpose of visualisation, we want predictions from our model to plot. To do this, we map again. This time, we use the itsadug function get_predictions() instead of the mgcv function bam(), but the underlying idea is the same. The function get_predictions() needs us to tell it what values we want predictions for. In this case, we want predictions for the full range of years of birth in our data set (1864-1981) and for both genders.\nThe following code stores the values we want predictions for in the to_predict object and then creates a prediction column using mutate() and map():\n\nto_predict &lt;- list(\n  \"yob\" = seq(from=1864, to=1981, by=1), # All years\n  \"Gender\" = c(\"M\", \"F\")\n) \n# BTW: Get prediction will just assume the average value for any predictors not\n# mentioned (in this case, Speech_rate).\n\nvowels &lt;- vowels %&gt;%\n  mutate(\n    prediction = map(\n      model, # This time we're applying the function to all the models.\n      # We again introduce the function with '~', and indicate where the model \n      # goes with '.x'.\n      ~ get_predictions(model = .x, cond = to_predict, print.summary = FALSE)\n    )\n  )\n\nSo what does a tibble of predictions look like?\n\nvowels$prediction[[1]] %&gt;%\n  head() %&gt;%\n  kable() %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\")\n\n\n\n\n\nGender\nyob\nSpeech_rate\nfit\nCI\n\n\n\n\nM\n1864\n4.4873\n501.5480\n27.00804\n\n\nF\n1864\n4.4873\n570.1079\n36.38487\n\n\nM\n1865\n4.4873\n501.0873\n26.68430\n\n\nF\n1865\n4.4873\n569.3392\n35.95156\n\n\nM\n1866\n4.4873\n500.6266\n26.36302\n\n\nF\n1866\n4.4873\n568.5705\n35.51985\n\n\n\n\n\n\n\nAs expected, we get a predicted value for each gender in each of the years spanned by the data.\n\n\nUnnest\nIn order to visualise the predictions of our models, we need to unnest this data set. We will do this in a slightly non-standard way, by making a new unnested tibble at this stage rather than modifying our original tibble again. But the same principles apply.\nIn this case, we need to select our identifying variables (Vowel and Formant) and the column with the data we want access to in a non-nested form (for us, prediction). We unnest as follows:\n\npredictions &lt;- vowels %&gt;%\n  select(\n    Vowel, Formant, prediction\n  ) %&gt;%\n  unnest(prediction)\n\npredictions %&gt;%\n  head() %&gt;%\n  kable() %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\")\n\n\n\n\n\nVowel\nFormant\nGender\nyob\nSpeech_rate\nfit\nCI\n\n\n\n\nDRESS\nF1_50\nM\n1864\n4.4873\n501.5480\n27.00804\n\n\nDRESS\nF1_50\nF\n1864\n4.4873\n570.1079\n36.38487\n\n\nDRESS\nF1_50\nM\n1865\n4.4873\n501.0873\n26.68430\n\n\nDRESS\nF1_50\nF\n1865\n4.4873\n569.3392\n35.95156\n\n\nDRESS\nF1_50\nM\n1866\n4.4873\n500.6266\n26.36302\n\n\nDRESS\nF1_50\nF\n1866\n4.4873\n568.5705\n35.51985\n\n\n\n\n\n\n\nOur tibble now has our predicted values along with the Vowel and Formant information which identifies which model they came from.\nNote that we have now thrown away our individual speaker information. This is often the case when visualising models of this sort as we are not interested in predicting the speech of this or that particular speaker in our data set. Rather, we want to say something about NZE speech in general.\nWe are now in a position to visualise changes in the overall NZE vowel space."
  },
  {
    "objectID": "posts/visualising-vowel-space-gamms/index.html#visualise-model-predictions-as-a-vowel-space",
    "href": "posts/visualising-vowel-space-gamms/index.html#visualise-model-predictions-as-a-vowel-space",
    "title": "Visualising Vowel Space Change with GAMs",
    "section": "Visualise Model Predictions as a Vowel Space",
    "text": "Visualise Model Predictions as a Vowel Space\nWe first produce a static plot using a standard ggplot approach and then produce an animation using gganimate.\nWe begin by defining a colour scheme (following Brand et al. 2021, again). These use html colour codes (plenty of explanations are available online).\n\nvowel_colours &lt;- c(\n  DRESS = \"#9590FF\",\n  FLEECE = \"#D89000\",\n  GOOSE = \"#A3A500\",\n  KIT = \"#39B600\",\n  LOT = \"#00BF7D\",\n  NURSE = \"#00BFC4\",\n  START = \"#00B0F6\",\n  STRUT = \"#F8766D\",\n  THOUGHT = \"#E76BF3\",\n  TRAP = \"#FF62BC\"\n)\n\nWe also need to reverse our previous use of pivot_longer(). Why? Well vowel spaces have F1 as the y-axis and F2 as the x-axis. This requires F1 and F2 to be distinct columns. To do this, we use pivot_wider(). This is made easier if we remove the columns we will not use for plotting first.\n\npredictions &lt;- predictions %&gt;%\n  select( # Remove unneeded variables\n    -Speech_rate,\n    -CI\n  ) %&gt;%\n  pivot_wider( # Pivot\n    names_from = Formant,\n    values_from = fit\n  )\n\npredictions %&gt;%\n  head() %&gt;%\n  kable() %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\")\n\n\n\n\n\nVowel\nGender\nyob\nF1_50\nF2_50\n\n\n\n\nDRESS\nM\n1864\n501.5480\n1657.465\n\n\nDRESS\nF\n1864\n570.1079\n1964.213\n\n\nDRESS\nM\n1865\n501.0873\n1659.390\n\n\nDRESS\nF\n1865\n569.3392\n1966.005\n\n\nDRESS\nM\n1866\n500.6266\n1661.316\n\n\nDRESS\nF\n1866\n568.5705\n1967.797\n\n\n\n\n\n\n\n\nStatic Plot\nThe plot we eventually produce is quite complex. Let’s start with depicting the data in two-dimensions using points and only plotting the female speakers.\n\npredictions %&gt;%\n  filter(\n    Gender == \"F\"\n  ) %&gt;%\n  ggplot(\n    aes(\n      x = F2_50,\n      y = F1_50,\n      colour = Vowel\n    )\n  ) +\n  geom_point()\n\n\n\n\n\n\n\n\nThis is the wrong way around for a vowel plot. So we reverse the x and y axes.\n\npredictions %&gt;%\n  filter(\n    Gender == \"F\"\n  ) %&gt;%\n  ggplot(\n    aes(\n      x = F2_50,\n      y = F1_50,\n      colour = Vowel\n    )\n  ) +\n  geom_point() +\n  scale_x_reverse() +\n  scale_y_reverse()\n\n\n\n\n\n\n\n\nIt’s also unclear which direction these changes are occurring. We need to swap out geom_point() for something directed. In this case, we use geom_path() and arrow().\n\npredictions %&gt;%\n  filter(\n    Gender == \"F\"\n  ) %&gt;%\n  ggplot(\n    aes(\n      x = F2_50,\n      y = F1_50,\n      colour = Vowel\n    )\n  ) +\n  geom_path(arrow = arrow(length = unit(5, \"mm\"))) +\n  scale_x_reverse() +\n  scale_y_reverse()\n\n\n\n\n\n\n\n\nWe can add labels to the start of each arrow and remove the legend. This is found by some to be a more clear vowel plot. To do this, we have to pick out the first observation of each vowel in a new tibble. This prevents us from labelling every point on the line.\n\nfirst_obs &lt;- predictions %&gt;%\n  group_by(Vowel, Gender) %&gt;%\n  slice(which.min(yob))\n\npredictions %&gt;%\n  filter(\n    Gender == \"F\"\n  ) %&gt;%\n  ggplot(\n    aes(\n      x = F2_50,\n      y = F1_50,\n      colour = Vowel,\n      label = Vowel # Add label to the aesthetics.\n    )\n  ) +\n  geom_path(\n    arrow = arrow(length = unit(5, \"mm\")),\n    show.legend = FALSE # Remove legend\n  ) +\n  geom_label(\n    # Note filtering as we are only dealing with female speakers now.\n    data = first_obs %&gt;% filter(Gender == \"F\"), \n    show.legend = FALSE # Again remove legend.\n  ) +\n  # Often need to use 'expansion' here to fit in 'THOUGHT'\n  scale_x_reverse(expand = expansion(add = 100)) + \n  scale_y_reverse()\n\n\n\n\n\n\n\n\nWe can use the faceting functions to plot both male and female data. We use the facet_grid() function. NB: this requires us to remove the filter() functions from the above.\n\npredictions %&gt;%\n  ggplot(\n    aes(\n      x = F2_50,\n      y = F1_50,\n      colour = Vowel,\n      label = Vowel\n    )\n  ) +\n  geom_path(\n    arrow = arrow(length = unit(5, \"mm\")),\n    show.legend = FALSE\n  ) +\n  geom_label(\n    data = first_obs, \n    show.legend = FALSE\n  ) +\n  scale_x_reverse(expand = expansion(add = 250)) + \n  scale_y_reverse() +\n  facet_grid(\n    cols = vars(Gender)\n  )\n\n\n\n\n\n\n\n\nThere are still a few shortcomings here. We have overlap of the labels, which are now a little large. We attempt to fix this, while also adding a call to labs() to properly label the plot.\n\npredictions %&gt;%\n  ggplot(\n    aes(\n      x = F2_50,\n      y = F1_50,\n      colour = Vowel,\n      label = Vowel\n    )\n  ) +\n  geom_path(\n    arrow = arrow(length = unit(2.5, \"mm\")), # Make arrows smaller\n    show.legend = FALSE\n  ) +\n  geom_label(\n    data = first_obs, \n    show.legend = FALSE,\n    size = 2.5, # Make labels smaller...\n    alpha = 0.7 # ...and slightly transparent.\n  ) +\n  scale_x_reverse(expand = expansion(add = 200)) + # less expanasion needed.\n  scale_y_reverse() +\n  facet_grid(\n    cols = vars(Gender)\n  ) +\n  labs(\n    title = \"Vowel Space Change in NZE (yob: 1864-1981)\",\n    x = \"First Formant (Hz)\",\n    y = \"Second Formant (Hz)\"\n  )\n\n\n\n\n\n\n\n\nThis figure is a good starting point for the kind of smaller adjustments needed to produce a good vowel space visualisation. In your own cases, a lot will depend on the specifics of the models and how much change is being depicted. In this case, it would be nice to make some of these lines a little less messy. Some of the problems here, for instance, the messy lines for start and strut in the male speakers, suggest that our models might be behaving a strangely. This is not a problem for this illustration, but some model criticism would be required in a real research project!\n\n\nAnimation\nOne problem with the static plot is that we can’t see variation in the rate of change (if any exists). Nor can we figure out, for any point on the line (apart from the start and end) which year it represents. This can be fixed with the gganimate package.\nWe use the gganimate function transition_reveal() at the end of our plot with yob as the variable we animate with. Our geom_label(), the text with the vowel name, will be the main item we animate. The labels will move around the vowel space over time. We change our geom_path() so that it has no arrows and simply traces where the label of the vowel moves.\nWe add a caption to our labs() in which we reference the gganimate variable frames_along. This allows us to show what year it is. Finally, a call to theme() lets us make this caption larger.\n\npredictions %&gt;%\n  ggplot(\n    aes(\n      x = F2_50,\n      y = F1_50,\n      colour = Vowel,\n      label = Vowel\n    )\n  ) +\n  geom_path(show.legend = FALSE) +\n  # NB: our labels just use the predictions dataframe now, so no need for the\n  # 'data = ' line.\n  geom_label( \n    show.legend = FALSE,\n    size = 2.5, \n    alpha = 0.7 \n  ) +\n  scale_x_reverse(expand = expansion(add = 200)) + \n  scale_y_reverse() +\n  facet_grid(\n    cols = vars(Gender)\n  ) +\n  labs(\n    title = \"Vowel Space Change in NZE (yob: 1864-1981)\",\n    x = \"First Formant (Hz)\",\n    y = \"Second Formant (Hz)\",\n    caption = 'Year of Birth: {round(frame_along, 0)}' \n  ) +\n  theme(\n    plot.caption = element_text(size = 14, hjust = 0)\n  ) +\n  transition_reveal(along = yob)\n\n`geom_path()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_path()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_path()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_path()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n\n\n\nSo there is it: a general structure for fitting models to visualise changes in vowel formants over time, and for plotting them within vowel space diagrams.\nAs I’ve noted multiple times above, the details will matter at every step in a real research project. Data quality assessment before modelling, sensible model structures, evaluation of model quality, and careful consideration of exactly what needs to be visualised are all necessary.\nThis is, however, plenty for a blog post!"
  },
  {
    "objectID": "posts/plotly-struggles/index.html",
    "href": "posts/plotly-struggles/index.html",
    "title": "A 3D PCA Visualisation with Plotly",
    "section": "",
    "text": "I am attempting to build a nice illustrative example of PCA which shows how it can produce interpretable, lower-dimensional representations of vowel formant data.\nOne well-trodden method of showing how PCA works is to start with three dimensions and go down to two dimensions.\nHeavy use of ggplot2 for the last year and a half has worn deep grooves into my brain. But ggplot2 does not offer options for 3D plots.\nA few extension packages exist to extend ggplot into three dimensions, but nothing stood out to me as I was looking around. I have also wanted to explore plotly a little more after playing around with plotly cytoscapes for another project.\nThe following example, which will be used for a larger project on PCA in the study of vowel covariation, took far more work than anticipated. I’ll present it in the next section and then run through some of the problems which occurred for me."
  },
  {
    "objectID": "posts/plotly-struggles/index.html#the-problem",
    "href": "posts/plotly-struggles/index.html#the-problem",
    "title": "A 3D PCA Visualisation with Plotly",
    "section": "",
    "text": "I am attempting to build a nice illustrative example of PCA which shows how it can produce interpretable, lower-dimensional representations of vowel formant data.\nOne well-trodden method of showing how PCA works is to start with three dimensions and go down to two dimensions.\nHeavy use of ggplot2 for the last year and a half has worn deep grooves into my brain. But ggplot2 does not offer options for 3D plots.\nA few extension packages exist to extend ggplot into three dimensions, but nothing stood out to me as I was looking around. I have also wanted to explore plotly a little more after playing around with plotly cytoscapes for another project.\nThe following example, which will be used for a larger project on PCA in the study of vowel covariation, took far more work than anticipated. I’ll present it in the next section and then run through some of the problems which occurred for me."
  },
  {
    "objectID": "posts/plotly-struggles/index.html#a-plotly-example",
    "href": "posts/plotly-struggles/index.html#a-plotly-example",
    "title": "A 3D PCA Visualisation with Plotly",
    "section": "A Plotly Example",
    "text": "A Plotly Example\nHere’s the code, with the Plotly interactive following.\n\nlibrary(tidyverse)\nlibrary(plotly)\n\n# Reorganising data\n\nload('onze_means.rda')\n\nonze_sub &lt;- onze_means %&gt;%\n  filter(\n    vowel %in% c(\"DRESS\", \"KIT\", \"TRAP\")\n  ) %&gt;%\n  select(\n    speaker, vowel, F1_50, yob, gender\n  )\n\nonze_to_pca &lt;- onze_sub %&gt;%\n  pivot_wider(\n    names_from = vowel,\n    values_from = F1_50\n  )\n\n# Running PCA\n\nonze_pca &lt;- prcomp(\n  onze_to_pca %&gt;% select(DRESS, KIT, TRAP),\n  scale=FALSE\n)\n\n# Extracting PCA information\n\n# Load scores for individuals (contained in onze_pca$x)\nonze_to_pca &lt;- onze_to_pca %&gt;%\n  mutate(\n    PC1 = onze_pca$x[, 1],\n    PC2 = onze_pca$x[, 2]\n  )\n\n# Extract centre. This will be used in the plot.\npca_center &lt;- onze_pca$center\n\ncenter_tibble &lt;- tibble(\n  \"DRESS\" = pca_center[[1]],\n  \"KIT\" = pca_center[[2]],\n  \"TRAP\" = pca_center[[3]]\n)\n\n# Extract loadings.\npca_loadings &lt;- onze_pca$rotation\n\n# Use the loadings and centre to find where each point sits along PC1 and PC2\n# when represented in the original 3D space.\nonze_to_pca &lt;- onze_to_pca %&gt;%\n  mutate(\n    PC1_DRESS = (PC1 * pca_loadings[1, 1]) + pca_center[[1]],\n    PC1_KIT = (PC1 * pca_loadings[2, 1]) + pca_center[[2]],\n    PC1_TRAP = (PC1 * pca_loadings[3, 1]) + pca_center[[3]],\n    PC2_DRESS = (PC2 * pca_loadings[1, 2]) + pca_center[[1]],\n    PC2_KIT = (PC2 * pca_loadings[2, 2]) + pca_center[[2]],\n    PC2_TRAP = (PC2 * pca_loadings[3, 2]) + pca_center[[3]],\n  )\n\n# Visualisation\n\nfig &lt;- plot_ly(\n  data = onze_to_pca,\n  x = ~DRESS,\n  y = ~KIT,\n  z = ~TRAP,\n  type = \"scatter3d\", # What kind of plot.\n  mode = \"markers\", # What kind of marking (something like geom in ggplot)\n  name = \"Speakers\", # This label will go in the legend.\n  marker = list(\n    size = 2,\n    opacity = 0.8,\n    color = \"gray\",\n    lines = list(\n      color = 'black',\n      width = 1\n    ),\n    showlegend = FALSE\n  )\n)\n\n# Add title and axis labels\nfig &lt;- fig %&gt;%\n  layout(\n    margin = list(\n      t = 90\n    ),\n    title = list(\n      text = \"Mean KIT, DRESS, and TRAP F1 for 100 ONZE Speakers\",\n      font = list(\n        size = 24,\n        family = \"Open Sans\"\n      )\n    ),\n    scene = list(\n      xaxis = list(\n        title = \"DRESS F1 (Hz)\"\n      ),\n      yaxis = list(\n        title = \"KIT F1 (Hz)\"\n      ),\n      zaxis = list(\n        title = \"TRAP F1 (Hz)\"\n      )\n    )\n  )\n\nfig &lt;- fig %&gt;%\n  # Add centre mark\n  add_trace(\n    x = ~DRESS,\n    y = ~KIT,\n    z = ~TRAP,\n    data = center_tibble,\n    type = 'scatter3d',\n    mode = 'markers',\n    marker = list(\n      size = 6,\n      opacity = 1,\n      color = 'red',\n      symbol = \"x\"\n      ),\n    name = \"Center\",\n    inherit = FALSE\n  ) %&gt;%\n  # Add PC1 line\n  add_trace(\n    data = onze_to_pca,\n    x = ~PC1_DRESS,\n    y = ~PC1_KIT,\n    z = ~PC1_TRAP,\n    type = 'scatter3d',\n    mode = 'lines',\n    name = 'PC1',\n    line = list(\n      color = \"#D55E00\",\n      width = 6\n    ),\n    inherit = FALSE\n  ) %&gt;%\n  # Add PC2 line.\n  add_trace(\n    data = onze_to_pca,\n    x = ~PC2_DRESS,\n    y = ~PC2_KIT,\n    z = ~PC2_TRAP,\n    type = 'scatter3d',\n    mode = 'lines',\n    name = 'PC2',\n    line = list(\n      color = \"#0072B2\",\n      width = 4\n    ),\n    inherit = FALSE\n  ) %&gt;%\n  layout(\n    scene = list(\n      xaxis = list(\n        title = \"DRESS F1 (Hz)\"\n      ),\n      yaxis = list(\n        title = \"KIT F1 (Hz)\"\n      ),\n      zaxis = list(\n        title = \"TRAP F1 (Hz)\"\n      )\n    )\n  )\n\nfig\n\n\n\n\n\nThe plot above can be rotated with the mouse and the legend on the right allows for different plot elements to be turned off. Double click ‘Speakers’ to see just the points for each speaker.\nWhat are these elements?\n\nThe points are the mean first formant values for dress, trap, and kit for 100 speakers from the Origins of New Zealand English (ONZE) corpus.\nThe ‘Center’ is the centre of the cloud of points.\nPC1 shows the first principal component derived from the data.\nPC2 shows the second principal component.\n\nThe idea here is to explain PCA in three dimensions as putting a cross in the centre of a cloud of points and then drawing the straight line which captures the most variation in the data. That is, to draw the straight line which is ‘inside’ the cloud of points for as much of its length as possible. We then create the second PC, by finding the line at right angles to the first line which captures the most possible variance.\nThe data can then be plotted against these two lines rather than the original three dimensions and the same idea extends to cases when we have many many more variables than three.\nIdeally, the principal components are interpretable. In this case, the first PC is a line from small values of all three variables to large values. That is, we can interpret it as capturing something like vocal tract length. Speakers with long vocal tracts tend to have lower first formant values.\nThe second formant is also interpretable. The ONZE corpus contains speakers born in the mid-nineteenth century all the way through to the 1980’s. In this time, dress and trap have raised, while kit has lowered. The line for PC2 (the blue line), runs from high values of dress and trap and low values of kit to low values of dress and trap and high values of kit. That is, it captures the structure in our data which comes from the structured change in vowel spaces which has occurred over time in New Zealand English."
  },
  {
    "objectID": "posts/plotly-struggles/index.html#lessons-learned",
    "href": "posts/plotly-struggles/index.html#lessons-learned",
    "title": "A 3D PCA Visualisation with Plotly",
    "section": "Lessons Learned",
    "text": "Lessons Learned\nI have not found the plotly documentation for R to be very easy to get my head into.\nA few things which caused me trouble and might help you:\n\nplotly is not ‘surly’. Surliness is a feature of the tidyverse packages: they complain. If you add an argument to a plotly function which it doesn’t recognise, it will often just ignore it without telling you. This is not great for people who like to use ‘colour’ rather than ‘color’ or ‘grey’ rather than ‘gray’.\nOften an initial plot is made with plot_ly() and then additional features are added to the plot with add_trace(). In this case, the points are added to the plot at the plot_ly() stage, and the lines and the red cross added using add_trace(). The first argument to plot_ly() is a data frame, but this is not the case for add_trace(). I have found being explicit by saying data = onze_to_pca rather than just having plot_ly(onze_to_pca ...), avoids confusion. It took me a long time to work out why my add_trace(onze_to_pca) was not functioning.\nIf the added element is different in type from the original plot, it is vital to add inherit = FALSE to any add_trace() calls. My initial attempts to add lines to the plot added both lines and points (‘markers’ in plotly speak) because I failed to add inherit = FALSE.\nAxis labelling for 3D plots occurs by modifying the scene rather than xaxis or yaxis etc. directly. See the call to layout() in the plot code.\n\nIt may be that there are obvious solutions to some of these problems. If so, please let me know!"
  },
  {
    "objectID": "posts/nzilbb-vowels-0-3-1/index.html",
    "href": "posts/nzilbb-vowels-0-3-1/index.html",
    "title": "nzilbb.vowels 0.3.1",
    "section": "",
    "text": "The nzilbb.vowels R package has just been released to CRAN. This is the first release on CRAN and contains a few modifications from the version which was available on GitHub and used in Wilson Black et al. (2022)."
  },
  {
    "objectID": "posts/nzilbb-vowels-0-3-1/index.html#plot_pc_vs-and-pc_flip",
    "href": "posts/nzilbb-vowels-0-3-1/index.html#plot_pc_vs-and-pc_flip",
    "title": "nzilbb.vowels 0.3.1",
    "section": "plot_pc_vs() and pc_flip()",
    "text": "plot_pc_vs() and pc_flip()\nThe model-to-PCA workflow for vocalic data can produce results which require a lot of mental ‘axis flipping’. First, the convention is to plot the axes in reverse in vowel space diagrams. Second, when two PCA analyses with one another, one might have a positive loadings where the other has negative loadings. But the sign of a PC is arbitrary.\nTo quickly solve the ‘loadings-to-vowel space’ problem, I’ve added a function called plot_pc_vs(), which plots a principal component generated by prcomp, princomp or nzilbb.vowels::pca_test() in the vowel space. For instance:\n\nonze_vowels &lt;- onze_vowels |&gt;  \n  # apply lobanov normalisation\n  lobanov_2() |&gt; \n  # put lobanov normalised values towards front of data frame.\n  relocate(speaker, vowel, F1_lob2, F2_lob2)\n\npca_data &lt;- onze_intercepts |&gt;\n  select(-speaker)\n\nonze_test &lt;- pca_test(\n  pca_data,\n  n = 500,\n  scale = TRUE,\n  variance_confint = 0.95,\n  loadings_confint = 0.9\n)\n\nplot_pc_vs(onze_vowels, onze_test, pc_no = 1, is_sig = TRUE) +\n  coord_fixed()\n\n\n\n\n\n\n\nFigure 1: Significant PC1 loadings for subset of ONZE speakers in vowel space.\n\n\n\n\n\nThe arrows in Figure 1 indicate relative size of movement, but do not indicate the exact magnitude of movement in the vowel space expected for a specific increase in PC score. The way of getting at this will vary in each research project and will depend on how any models have been used. The purpose of plot_pc_vs() is to provide a quick way of getting from PC loadings to movement in the vowel space.\npc_flip() allows you to flip a named PC or to specify a variable which one wants to be positive. So, for instance, if you know you want trap F1 to be positive in PC1, you’d do the following:\n\nonze_test &lt;- pc_flip(onze_test, pc_no = 1, flip_var = \"F1_TRAP\")\n\nAs wil plot_pc_vs(), pc_flip() works with prcomp, princomp or nzilbb.vowels::pca_test().\nWe can use plot_pc_vs() again to see that this has worked.\n\nplot_pc_vs(onze_vowels, onze_test, pc_no = 1, is_sig = TRUE) +\n  coord_fixed()\n\n\n\n\n\n\n\nFigure 2: Significant PC1 loadings for subset of ONZE speakers in vowel space (flipped)."
  },
  {
    "objectID": "posts/nzilbb-vowels-0-3-1/index.html#mds_test",
    "href": "posts/nzilbb-vowels-0-3-1/index.html#mds_test",
    "title": "nzilbb.vowels 0.3.1",
    "section": "mds_test()",
    "text": "mds_test()\nIn the course of Sheard et al. (2024), we build a testing function for choosing a number of dimensions in Multidimensional Scaling (MDS) by analogy with pca_test(). The function calculates the reduction in success achieved by adding an additional dimension to an MDS analysis for both bootstrapped and permuted versions of a similarity matrix. Here’s what it, and the plot_mds_test() function look like in practice:\n\nmds_res &lt;- mds_test(\n  sim_matrix,\n  n_boots = 50,\n  n_perms = 50,\n  test_dimensions = 5,\n  principal = TRUE,\n  mds_type = \"ordinal\",\n  spline_degree = 2,\n  spline_int_knots = 2\n)\nplot_mds_test(mds_res)\n\n\n\n\n\n\n\nFigure 3: Plot of MDS test results.\n\n\n\n\n\nIn Figure 3, the black crosses indicate the stress reduction from adding an additional dimension for a given similarity matrix. The red box and whisker plot indicates the reduction across a series of bootstraps and the blue indicates the same for permuted versions of the data. We’ve found that accepting the number of dimensions up to and including the first where the two distributions align seems to perform well, but we are still experimenting! In the case of this (simulated) data, we’d go with two dimensions."
  },
  {
    "objectID": "posts/nzilbb-vowels-0-3-1/index.html#website",
    "href": "posts/nzilbb-vowels-0-3-1/index.html#website",
    "title": "nzilbb.vowels 0.3.1",
    "section": "Website",
    "text": "Website\nThe package now has a pkgdown website at https://nzilbb.github.io/nzilbb_vowels. Have a look to see the rest of the documentation."
  },
  {
    "objectID": "posts/significance-testing-for-vowel-space-gamms/index.html",
    "href": "posts/significance-testing-for-vowel-space-gamms/index.html",
    "title": "Model Check and Significance Testing for Vowel Space GAMMs",
    "section": "",
    "text": "In a previous post, I set out a workflow for fitting and visualising generalised additive mixed models for F1 and F2 of a series of vowels and using these to visualise change over an entire vowel space.\nIn that post, I waved off the issue of evaluating individual models. In this post I’d like to fill out two aspects of this and link them to the workflow I presented in the previous post. The first is the use of the gam.check function to work out if your smooths enable sufficient ‘wiggliness’ for your data and whether the assumption of normally distributed residuals holds. The second is to show how information about model significance can be extracted both in terms of significance of change in a given vowels trajectory over time and in terms of the difference between the trajectories of, say, men and women (following the recent advice of Márton Sóskuthy.\nWe’ll very quickly run through fitting models using the nest and mutate approach (using purrr), look at the use of gam.check within this framework, and then turn to significance testing. I’ll conclude by adding an indication of significance to the static and animated vowel space plots presented in the previous post.\n\n\n\nNZBC Presents Logo (source)"
  },
  {
    "objectID": "posts/significance-testing-for-vowel-space-gamms/index.html#introduction",
    "href": "posts/significance-testing-for-vowel-space-gamms/index.html#introduction",
    "title": "Model Check and Significance Testing for Vowel Space GAMMs",
    "section": "",
    "text": "In a previous post, I set out a workflow for fitting and visualising generalised additive mixed models for F1 and F2 of a series of vowels and using these to visualise change over an entire vowel space.\nIn that post, I waved off the issue of evaluating individual models. In this post I’d like to fill out two aspects of this and link them to the workflow I presented in the previous post. The first is the use of the gam.check function to work out if your smooths enable sufficient ‘wiggliness’ for your data and whether the assumption of normally distributed residuals holds. The second is to show how information about model significance can be extracted both in terms of significance of change in a given vowels trajectory over time and in terms of the difference between the trajectories of, say, men and women (following the recent advice of Márton Sóskuthy.\nWe’ll very quickly run through fitting models using the nest and mutate approach (using purrr), look at the use of gam.check within this framework, and then turn to significance testing. I’ll conclude by adding an indication of significance to the static and animated vowel space plots presented in the previous post.\n\n\n\nNZBC Presents Logo (source)"
  },
  {
    "objectID": "posts/significance-testing-for-vowel-space-gamms/index.html#model-fit-again",
    "href": "posts/significance-testing-for-vowel-space-gamms/index.html#model-fit-again",
    "title": "Model Check and Significance Testing for Vowel Space GAMMs",
    "section": "Model Fit (Again)",
    "text": "Model Fit (Again)\nI’ll rush over this part, given that it is covered in the previous post.\nThree things are worth noting:\n\nI’m bringing in the data for the models using my own package nzilbb.vowels. This is very much work in progress and I don’t promise anything about it’s future versions! I will endeavour to keep this post functional.\nI’ll point out where the structure differs from that used in the previous post.\nUnlike the previous post, I’m using raw token data from each speaker rather than just taking their mean values. This makes the data a little noisier and the models a little harder to fit.\n\nLet’s load some libraries:\n\n# Required tidyverse packages \n# (I occasionally try to do without 'library(tidyverse)')\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(glue)\nlibrary(ggplot2)\nlibrary(gganimate)\n\n# gtools library for generating pval stars.\nlibrary(gtools)\n\n# mgcv for gamms, itsadug for gamm visualisation\nlibrary(mgcv)\nlibrary(itsadug)\n\n# nzilbb.vowels for data and some helper functions\n# To install, use remotes::install_github('JoshuaWilsonBlack/nzilbb_vowels')\nlibrary(nzilbb.vowels)\n\nOur data is built in to nzilbb.vowels under the name onze_vowels. Our modelling requires that we represent our factors as factor vectors rather than character vectors.\n\nonze_data &lt;- as_tibble(onze_vowels) %&gt;%\n  mutate(\n    across(.cols = c('speaker', 'vowel', 'word'), .fn = as.factor),\n  )\n\nThe next steps will fly by without much comment. We apply Lobanov 2.0 filtering and remove the old F1 and F2 values for convenience.\n\nonze_data &lt;- onze_data %&gt;%\n  lobanov_2() %&gt;%\n  select(-(F1_50:F2_50))\n\nJoining with `by = join_by(speaker, vowel)`\n\n\nNext, we pivot the data so that the normalised F1 and F2 data appears in a single column, group the tibble and nest so that we have a row for each model we want to fit.\n\nonze_models &lt;- onze_data %&gt;%\n   pivot_longer(\n     cols = F1_lob2:F2_lob2,\n     names_to = \"formant_type\",\n     values_to = \"formant_value\"\n   ) %&gt;%\n  group_by(vowel, formant_type) %&gt;%\n  nest()\n\nNow it’s time to fit some models.\n\nonze_models &lt;- onze_models %&gt;%\n  mutate(\n    model = map(\n      data,\n      ~ bam(\n        formant_value ~ gender + s(yob, by = gender) # main effects\n          + s(speech_rate) + # control variable\n          s(speaker, bs = 're') + s(word, bs = \"re\"), # random effects\n        data = .x,\n        discrete = T,\n        nthreads = 16 # I'm on a silly computer right now. \n        # Perhaps reduce nthreads to 4.\n      )\n    )\n  )"
  },
  {
    "objectID": "posts/significance-testing-for-vowel-space-gamms/index.html#using-gam.check",
    "href": "posts/significance-testing-for-vowel-space-gamms/index.html#using-gam.check",
    "title": "Model Check and Significance Testing for Vowel Space GAMMs",
    "section": "Using gam.check",
    "text": "Using gam.check\nThe function gam.check from mgcv tells us a lot about a GAMM. Here it is for the first GAMM in our tibble. This can be done by taking the first object from the model column.\n\ngam.check(onze_models$model[[1]])\n\n\n\n\n\n\n\n\n\nMethod: fREML   Optimizer: perf chol\n$grad\n[1] -2.459495e-05 -2.930034e-08 -4.469310e-05  1.003085e-05 -7.541875e-07\n[6]  1.565537e-05\n\n$hess\n           [,1]          [,2]          [,3]          [,4]          [,5]\n   2.459335e-05 -5.261901e-09 -2.823390e-11 -1.002578e-05  7.548775e-07\n  -5.261901e-09  9.229684e-02 -4.556448e-07  1.542197e-01 -6.173866e-03\n  -2.823390e-11 -4.556448e-07  4.469058e-05 -6.994732e-06 -1.076207e-06\n  -1.002578e-05  1.542197e-01 -6.994732e-06  3.353799e+01 -2.782142e-02\n   7.548775e-07 -6.173866e-03 -1.076207e-06 -2.782142e-02  1.462190e+01\nd -1.561899e-05 -3.098834e-01 -8.006339e-06 -3.960741e+01 -3.784472e+01\n           [,6]\n  -1.561899e-05\n  -3.098834e-01\n  -8.006339e-06\n  -3.960741e+01\n  -3.784472e+01\nd  3.468500e+03\n\nModel rank =  621 / 621 \n\nBasis dimension (k) checking results. Low p-value (k-index&lt;1) may\nindicate that k is too low, especially if edf is close to k'.\n\n                   k'    edf k-index p-value  \ns(yob):genderF   9.00   1.00    1.01   0.815  \ns(yob):genderM   9.00   1.62    1.01   0.835  \ns(speech_rate)   9.00   1.00    0.96   0.015 *\ns(speaker)     100.00  79.21      NA      NA  \ns(word)        492.00  75.69      NA      NA  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe get:\n\nA qq-plot and a histogram of residuals. Both suggest heavy tails. This seems to be quite common with formant data. One option to deal with this, although it increases computation load, is to use a scaled t distribution as our residual model. This can be done by adding family = scat(link='identity')) to the arguments of the bam function. I won’t explore this option in this post.\nTwo scatter plots of residuals. The linear predictor is the one to look at. There is no obvious pattern in this case. This is good!\nConsole output within which the main thing to check is the ‘Basis dimension (k) checking results’. These indicate the possible degrees of freedom (k') which each smooth is provided by the model specification and the effective degrees of freedom (edf) of the smooth which has been fit to the data. We want to avoid the combination of low p-values and edf close to k'. The current output doesn’t have anything to worry about here either. If edf gets close to k' we can increase the possible ‘wiggliness’ of the relevant term by adding the argument, say, k = 11 (which is one higher than the default value of 10) to the relevant smooth specification. When fitting the same model formula to multiple datasets, we need to pick a k value which works for all. Practically speaking, the only cost to increasing k is computational.\n\ngam.check does not return a value. So we can’t save it’s results using the mutate and map approach used above. I suggest walking through the data in the following way. This will output the plots and console output for each of the models in turn. The output is not given here to avoid this post becoming very long.\n\nwalk2(\n  str_c(onze_models$vowel, '_', onze_models$formant_type),\n  onze_models$model,\n  ~ {\n    print(.x)\n    gam.check(.y)\n  }\n)\n\nOne downside of the approach to fitting multiple models I’m suggesting here is that the same structure has to be used for all of them. The use of gam.check here is odd insofar as we are testing a series of particular models, but can only modify the general structure which we have set to apply to each model. It may be that there is a ‘tidymodelling’ approach which enables tweaking of each model. I’d be keen to hear of any such thing.\nAs always in statistics, you can’t just rely on a one-size-fits-all recipe. Reflection on the nature of your own data should always be involved in your selection of ‘k’."
  },
  {
    "objectID": "posts/significance-testing-for-vowel-space-gamms/index.html#significance-testing",
    "href": "posts/significance-testing-for-vowel-space-gamms/index.html#significance-testing",
    "title": "Model Check and Significance Testing for Vowel Space GAMMs",
    "section": "Significance Testing",
    "text": "Significance Testing\nWe have produced models of normalised first and second formant values for New Zealand English monophthongs over a long span of time. We’ve fit distinct model for each vowel and formant type pair (a model for dress F1, a model for dress F2, &c. &c…). Within each of these models, we fit a distinct smooth for the two gender categories in the data.\n\nP-Values for By-Factor Smooths from the Model Summary\nThe simplest method, if we are interested in which trajectories through the vowel space represent statistically significant changes is to extract p-values from the model summaries produced by the summary function.\nIn this case, we say that a trajectory is significant if either the p-value for the F1 smooth or the F2 smooth is less than 0.05. This kind of disjunctive use of p-values leads to false positives. So, for similar reasons to those presented in the Sóskuthy paper linked above, it is wise to apply the Bonferroni correction here. Since two p-value are involved, we simply divide our threshold for significance by 2, to give a new threshold of 0.0025.\nHere’s what the output of summary applied to one of our models looks like (I’ve picked one with significant effects on year of birth).\n\nmodel_index &lt;- 3\npaste0(\"Vowel = \", onze_models$vowel[[model_index]])\n\n[1] \"Vowel = FLEECE\"\n\npaste0(\"Formant type = \", onze_models$formant_type[[model_index]])\n\n[1] \"Formant type = F1_lob2\"\n\nsummary(onze_models$model[[3]])\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nformant_value ~ gender + s(yob, by = gender) + s(speech_rate) + \n    s(speaker, bs = \"re\") + s(word, bs = \"re\")\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.03870    0.02515 -41.306  &lt; 2e-16 ***\ngenderM     -0.15620    0.02947  -5.301 1.17e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                   edf  Ref.df      F p-value    \ns(yob):genderF   1.533   1.597 65.436  &lt;2e-16 ***\ns(yob):genderM   2.684   2.795 39.972  &lt;2e-16 ***\ns(speech_rate)   1.000   1.000  1.924   0.165    \ns(speaker)      74.509  96.000  7.205  &lt;2e-16 ***\ns(word)        215.748 734.000  1.968  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.221   Deviance explained =   24%\nfREML =  10133  Scale est. = 0.30672   n = 11896\n\n\nThis is the model for fleece F1. The parametric coefficients show the intercept terms for the women ((Intercept)) and the genderM term for the men. This gives the average (normalised) value for fleece F1 for the women and the difference between this and the average for the men. The difference between these values comes out as significant. But what we’re really interested in is whether they differ across the year of birth variable.\nTo see this, we look at the smooth terms. As the heading suggests, these are ‘approximate’ significances. For our purposes, this just means there’s ‘mathematical dragons lurking about here. All we ought to know is that, if a p-value is ’quite close’ to our cut off, we should report that. The p-values we are looking at for this model do not have this problem. We’re looking at the values for s(yob):genderF and s(yob):genderM. In both cases, they are less than \\(2\\times10^{-16}\\).\nWhile we’re at it, let’s plot these smooths using the plot_smooth function from itsadug.\n\nplot_smooth(\n  onze_models$model[[model_index]], \n  view=\"yob\", \n  plot_all = \"gender\",\n  main = \"FLEECE F1 Year of Birth Smooths\",\n  rug = TRUE\n)\n\nSummary:\n    * gender : factor; set to the value(s): F, M. \n    * yob : numeric predictor; with 30 values ranging from 1864.000000 to 1981.000000. \n    * speech_rate : numeric predictor; set to the value(s): 4.5576. \n    * speaker : factor; set to the value(s): IA_f_527. (Might be canceled as random effect, check below.) \n    * word : factor; set to the value(s): word_12921. (Might be canceled as random effect, check below.) \n    * NOTE : The following random effects columns are canceled: s(speaker),s(word)\n \n\n\n\n\n\nSmooths for males and females from FLEECE F1 model.\n\n\n\n\nThere’s no way to tell from the summary output what the nature of the two smooths is. We will turn to the question of whether the two smooths are significantly different from one another in moment. But with the current model, we can see that both a significantly different from 0.\nIn order for us to access the p-values for s(yob):genderM and s(yob):GenderF for all of our models at once, we need to know a bit more about how the output of summary is stored by R: summary, when applied to a GAMM, produces a named list with information about the GAMM (run ?summary.gam to see the details). We need the first two smooth p-values. These are stored with the name s.pv.\nTo extract this information for each model, we create a new column which extracts the p-values for the smooths from the summary of each GAMM, and then columns for each of the p-values we are interested in. This can be done as follows:\n\nonze_models &lt;- onze_models %&gt;%\n  mutate(\n    model_smooth_pvs = map(\n      model,\n      ~ summary(.x)$s.pv\n    ),\n    genderF_smooth_pv = map_dbl(\n      model_smooth_pvs,\n      ~ .x[[1]]\n    ),\n    genderM_smooth_pv = map_dbl(\n      model_smooth_pvs,\n      ~ .x[[2]]\n    )\n  ) %&gt;%\n  select(-model_smooth_pvs)\n\nNotice the use of map_dbl, which ensures that the resulting column is numerical rather than a list column. If anyone reading this has any idea of how to generate approximate p-values for the smooth terms in these models without running summary, I’d greatly appreciate it! This block takes a very long time to run.\nWe can look at the trajectories which appear as significant for either the males or females using the following code:\n\nonze_models %&gt;%\n  select(-model, -data) %&gt;% # Sometimes it's good to exclude the columns with \n  # large objects. \n  filter(genderF_smooth_pv &lt; 0.05 | genderM_smooth_pv &lt; 0.05)\n\n# A tibble: 20 × 4\n# Groups:   vowel, formant_type [20]\n   vowel   formant_type genderF_smooth_pv genderM_smooth_pv\n   &lt;fct&gt;   &lt;chr&gt;                    &lt;dbl&gt;             &lt;dbl&gt;\n 1 THOUGHT F1_lob2             0.237            0.00000107 \n 2 THOUGHT F2_lob2             0.0646           0.00454    \n 3 FLEECE  F1_lob2             0                0          \n 4 FLEECE  F2_lob2             0.00143          0          \n 5 KIT     F1_lob2             0                0          \n 6 KIT     F2_lob2             0                0          \n 7 DRESS   F1_lob2             0                0          \n 8 DRESS   F2_lob2             0.0454           0.000601   \n 9 GOOSE   F1_lob2             0                0.00000863 \n10 GOOSE   F2_lob2             0.000124         0          \n11 TRAP    F1_lob2             0                0          \n12 TRAP    F2_lob2             0.000969         0.254      \n13 START   F1_lob2             0.00438          0.000000694\n14 START   F2_lob2             0.0269           0.0154     \n15 STRUT   F1_lob2             0.0000647        0          \n16 STRUT   F2_lob2             0                0.000000718\n17 NURSE   F1_lob2             0                0          \n18 NURSE   F2_lob2             0.00000633       0.00000139 \n19 LOT     F1_lob2             0.00136          0.428      \n20 LOT     F2_lob2             0.00000706       0          \n\n\nEach is ‘significant’ by the 0.05 threshold for one gender or the other.\n\n\nPlotting Significance of By-Factor Smooths in Vowel Space\nHere, I again copy the steps in the previous post. The only difference here is that we are going to have information about significance along with our predicted formant values.\nWe generate model predictions from each model:\n\nto_predict &lt;- list(\n  \"yob\" = seq(from=1864, to=1981, by=1), # All years\n  \"gender\" = c(\"M\", \"F\")\n) \n# BTW: Get prediction will just assume the average value for any predictors not\n# mentioned (in this case, Speech_rate).\n\nonze_models &lt;- onze_models %&gt;%\n  mutate(\n    prediction = map(\n      model, # This time we're applying the function to all the models.\n      # We again introduce the function with '~', and indicate where the model \n      # goes with '.x'.\n      ~ get_predictions(model = .x, cond = to_predict, print.summary = FALSE)\n    )\n  )\n\nWe then unnest the predictions, making sure to keep our p-value information.\n\nonze_predictions &lt;- onze_models %&gt;%\n  select(\n    vowel, formant_type, genderF_smooth_pv, genderM_smooth_pv, prediction\n  ) %&gt;%\n  unnest(prediction)\n\nWe still need to pivot our data so that we have distinct columns for the fit and the p-value for each formant type.\n\nonze_predictions &lt;- onze_predictions %&gt;%\n  mutate(\n    p_value = if_else(gender == \"F\", genderF_smooth_pv, genderM_smooth_pv)\n  ) %&gt;%\n  select( # Remove unneeded variables\n    -speech_rate,\n    -CI,\n    -genderF_smooth_pv,\n    -genderM_smooth_pv\n  ) %&gt;%\n  pivot_wider( # Pivot\n    names_from = formant_type,\n    values_from = c(fit, p_value)\n  )\n\nOur significance test for each vowel is to test whether either of the p-values for the vowel is less than 0.025. We make a new column to track this.\n\nonze_predictions &lt;- onze_predictions %&gt;%\n  mutate(\n    is_sig = (p_value_F1_lob2 &lt; 0.025 | p_value_F2_lob2 &lt; 0.025),\n  )\n\nLet’s have a look at which of these turn up as significant:\n\nonze_predictions %&gt;%\n  group_by(vowel, gender) %&gt;%\n  summarise(is_sig = first(is_sig)) %&gt;%\n  arrange(is_sig)\n\n`summarise()` has grouped output by 'vowel'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 20 × 3\n# Groups:   vowel [10]\n   vowel   gender is_sig\n   &lt;fct&gt;   &lt;fct&gt;  &lt;lgl&gt; \n 1 THOUGHT F      FALSE \n 2 DRESS   M      TRUE  \n 3 DRESS   F      TRUE  \n 4 FLEECE  M      TRUE  \n 5 FLEECE  F      TRUE  \n 6 GOOSE   M      TRUE  \n 7 GOOSE   F      TRUE  \n 8 KIT     M      TRUE  \n 9 KIT     F      TRUE  \n10 LOT     M      TRUE  \n11 LOT     F      TRUE  \n12 NURSE   M      TRUE  \n13 NURSE   F      TRUE  \n14 START   M      TRUE  \n15 START   F      TRUE  \n16 STRUT   M      TRUE  \n17 STRUT   F      TRUE  \n18 THOUGHT M      TRUE  \n19 TRAP    M      TRUE  \n20 TRAP    F      TRUE  \n\n\nOf our 10 vowels, the only one which does not come up as significant is thought for the female speakers.\nWe will visualise significance in two ways. The first is to decrease the transparency of any vowel which does not meet our significance threshold, the second is to add to our vowel labels and indication of the p-values for F1 and F2 for each vowel. We’ll do this by adding a tag of the form ’(** | *)’ where the first set of stars indicates the magnitude of the F1 p-value and the second indicates the magnitude of the F2 p-value. We’ll use the usual stars for p-values. These can be conveniently generated using the stars.pval function from the gtools package.\nWe create the labels as follows:\n\nonze_predictions &lt;- onze_predictions %&gt;%\n  mutate(\n    F1_stars = stars.pval(p_value_F1_lob2),\n    F2_stars = stars.pval(p_value_F2_lob2),\n    vowel_label = glue(\n      \"{vowel}\\n({F1_stars}|{F2_stars})\"\n    )\n  )\n\nNow we apply the static plot code from the previous post.\n\nfirst_obs &lt;- onze_predictions %&gt;%\n  group_by(vowel, gender) %&gt;%\n  slice(which.min(yob))\n\nstatic_plot &lt;- onze_predictions %&gt;%\n  ggplot(\n    aes(\n      x = fit_F2_lob2,\n      y = fit_F1_lob2,\n      colour = vowel,\n      label = vowel_label,\n      alpha = is_sig\n    )\n  ) +\n  geom_path(\n    arrow = arrow(length = unit(2.5, \"mm\")), # Make arrows smaller\n    show.legend = FALSE\n  ) +\n  geom_label(\n    data = first_obs, \n    show.legend = FALSE,\n    size = 2, # Make labels small,\n    alpha = 0.7\n  ) +\n  scale_x_reverse(expand = expansion(add = 0.7)) + # less expanasion needed.\n  scale_y_reverse() +\n  scale_alpha_manual(values = c('FALSE' = 0.2, 'TRUE' = 1)) +\n  facet_grid(\n    cols = vars(gender)\n  ) +\n  labs(\n    title = \"Vowel Space Change in NZE (yob: 1864-1981)\",\n    x = \"Second Formant (Lobanov 2.0)\",\n    y = \"First Formant (Lobanov 2.0)\"\n  )\n\nstatic_plot\n\n\n\n\nONZE by-gender smooths, with significance indicated.\n\n\n\n\nThe plot here creates somewhat unwieldy labels. The advantage of these is that they give us a bit more detail than merely ‘is significant’ or ‘is not significant’. This is particularly important given that these plots will typically be used in exploratory rather than confirmatory contexts. It is also important that we provide some indication of the specific p-values. Those which are near 0.05 should be treated with a bit of scepticism.\nAnd now, the same plot but animated:\n\nonze_anim &lt;- onze_predictions %&gt;%\n  ggplot(\n    aes(\n      x = fit_F2_lob2,\n      y = fit_F1_lob2,\n      colour = vowel,\n      label = vowel_label,\n      alpha = is_sig\n    )\n  ) +\n  geom_path(show.legend = FALSE) +\n  # NB: our labels just use the predictions dataframe now, so no need for the\n  # 'data = ' line.\n  geom_label( \n    show.legend = FALSE,\n    size = 2.5\n  ) +\n  scale_x_reverse(expand = expansion(add = 0.5)) + \n  scale_y_reverse() +\n  facet_grid(\n    cols = vars(gender)\n  ) +\n  labs(\n    title = \"Vowel Space Change in NZE (yob: 1864-1981)\",\n    x = \"Second Formant (Lobanov 2.0)\",\n    y = \"First Formant (Lobanov 2.0)\",\n    caption = 'Year of Birth: {round(frame_along, 0)}' \n  ) +\n  theme(\n    plot.caption = element_text(size = 14, hjust = 0)\n  ) +\n  transition_reveal(along = yob)\n\nanimate(onze_anim, end_pause = 10)\n\nWarning: Using alpha for a discrete variable is not advised.\n\n\n`geom_path()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_path()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_path()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_path()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n\n\n\nHere the transparent applies to both the labels and the paths.\nYou might think that we should be applying some kind of multiple comparisons correction for the fact we are looking at 20 distinct statistical models! That would be the case if we were testing the hypothesis that there is some significant effect in one of the vowels we are looking at. I’d suggest that that is not an interesting hypothesis! What we’re trying to get at with these tools is a sense of systematic change in the data. If we wanted to turn this into a severe hypothesis test, we would need a much more worked out hypothesis which covered both which vowels we expect to have significant effects in the vowel space and what we expect those effects to be.\n\n\nP-Values for Difference Smooths\nIf we want to know whether differences between the trajectories are significant, then we need a different model structure. Rather than fitting distinct smooths to the two levels of our gender variable, we need to fit a smooth for the female speakers and a difference smooth for the males. This smooth represents, as the name suggests, the difference between the shapes of the smooths for the male and female speakers.\nThe difference smooth structure is implemented by mgcv when we use ordered factors with contrast-treatment coding. This is done in R as follows:\n\nonze_data &lt;- onze_data %&gt;%\n  mutate(\n    gender = as.ordered(gender)\n  )\n\ncontrasts(onze_data$gender) &lt;- 'contr.treatment'\n\nWe also have to change the model structure slightly. The difference between the needed model structure the structure used for by-factor smooths is that with difference smooths on a variable \\(x\\) by some factor \\(y\\), the model formula looks like y + s(x) + s(x, by=y) whereas if a smooth is fit to each level of the factor \\(y\\) independently the formula looks like y + s(x, by=y). The former structure is not actually identifiable if you do not have contrast coded ordered factors. It’s important to be clear whether you are fitting by-factor level smooths or difference smooths!\nWe fit new models. Here, to avoid the models dataframe exploding, I’ll just replace the old ones. In a real research project, you will probably want to save the original models before you do this.\n\nonze_models &lt;- onze_data %&gt;%\n  # Rerun the nesting steps.\n  pivot_longer(\n     cols = F1_lob2:F2_lob2,\n     names_to = \"formant_type\",\n     values_to = \"formant_value\"\n   ) %&gt;%\n  group_by(vowel, formant_type) %&gt;%\n  nest() %&gt;%\n  # Rerun modelling with new structure\n  mutate(\n    model = map(\n      data,\n      ~ bam(\n        formant_value ~ gender + s(yob) + s(yob, by = gender) # main effects\n          + s(speech_rate) + # control variable\n          s(speaker, bs = 're') + s(word, bs = \"re\"), # random effects\n        data = .x,\n        discrete = T,\n        nthreads = 16 \n      )\n    )\n  )\n\nWe skip checking the model assumptions here. The same points as above apply.\nLet’s have a look at a model summary for the same example as before (fleece) F1).\n\nsummary(onze_models$model[[model_index]])\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nformant_value ~ gender + s(yob) + s(yob, by = gender) + s(speech_rate) + \n    s(speaker, bs = \"re\") + s(word, bs = \"re\")\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.03295    0.02509  -41.17  &lt; 2e-16 ***\ngenderM     -0.15884    0.02947   -5.39 7.19e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                   edf  Ref.df      F p-value    \ns(yob)           2.093   2.180 45.640  &lt;2e-16 ***\ns(yob):genderM   2.072   2.161  1.415   0.274    \ns(speech_rate)   1.000   1.000  2.034   0.154    \ns(speaker)      74.459  96.000  7.253  &lt;2e-16 ***\ns(word)        215.736 734.000  1.969  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.221   Deviance explained =   24%\nfREML =  10133  Scale est. = 0.30672   n = 11896\n\n\nThe story we tell here is a bit different than the one we offered above. We have to account for both the parametric terms and the smooth terms. In the parametric coefficients, we see that the female speakers have an average F1 of -1.03295 in our Lobanov 2.0 normalised values. The genderM coefficient tells us that the male speakers have a value of which is smaller than the female speakers by 0.15884. That is, the male average is -1.19179. That is, in terms of the vowel space, their average fleece vowel is higher.\nThe smooth terms then tell us about the shape of the change in male and female speakers over time. We see that s(yob), the smooth for female speakers, is ‘statistically significant’. That is, there is change over time in this variable for our female speakers. The coefficient s(yob):genderM now indicates the difference in shape of the change over time between our female and male speakers. It is not statistically significant in this case. That is, the shape of the change over time could be the same for both speakers. This is quite different from the interpretation of s(yob):genderM in our previous model structure.\nThis case is interesting in that it looks intuitively from the animation in the previous section, that female speakers are leading the change in the height of fleece, with the male speakers following after a delay. The model, however, is representing this as males being across-the-board higher in their fleece vowel, while potentially following the same shape as the female speakers.\nWhen thinking about change over time in the vowel space, we might be interested in differences in shape between men and women but not in across-the-board differences in height. With that in mind, we can plot those trajectories which have significant differences in shape across male and female speakers.\nFollowing the same pattern as above, we say that a vowel has a significant difference in shape between male and female speakers if one or the other formant values has an s(yob):genderM p-value less than 0.0025. We follow the previous steps without much comment:\n\nonze_models &lt;- onze_models %&gt;%\n  mutate(\n    diff_smooth_pv = map_dbl(\n      model,\n      ~ summary(.x)$s.pv[[2]]\n    )\n  )\n\nonze_models &lt;- onze_models %&gt;%\n  mutate(\n    prediction = map(\n      model, \n      ~ get_predictions(model = .x, cond = to_predict, print.summary = FALSE)\n    )\n  )\n\nonze_predictions &lt;- onze_models %&gt;%\n  select(\n    vowel, formant_type, diff_smooth_pv, prediction\n  ) %&gt;%\n  unnest(prediction)\n\nonze_predictions &lt;- onze_predictions %&gt;%\n  select( # Remove unneeded variables\n    -speech_rate,\n    -CI\n  ) %&gt;%\n  pivot_wider( # Pivot\n    names_from = formant_type,\n    values_from = c(fit, diff_smooth_pv)\n  )\n\nonze_predictions &lt;- onze_predictions %&gt;%\n  mutate(\n    is_sig = (diff_smooth_pv_F1_lob2 &lt; 0.025 | diff_smooth_pv_F2_lob2 &lt; 0.025),\n  )\n\nWe now create labels and produce the static plot. Code which is changed from above is indicated in comments.\n\nonze_predictions &lt;- onze_predictions %&gt;%\n  mutate(\n    F1_stars = stars.pval(diff_smooth_pv_F1_lob2),\n    F2_stars = stars.pval(diff_smooth_pv_F2_lob2),\n    vowel_label = glue(\n      \"{vowel}\\n({F1_stars}|{F2_stars})\"\n    )\n  )\n\nfirst_obs &lt;- onze_predictions %&gt;%\n  group_by(vowel, gender) %&gt;%\n  slice(which.min(yob))\n\nstatic_plot &lt;- onze_predictions %&gt;%\n  ggplot(\n    aes(\n      x = fit_F2_lob2,\n      y = fit_F1_lob2,\n      colour = vowel,\n      label = vowel_label,\n      alpha = is_sig\n    )\n  ) +\n  geom_path(\n    arrow = arrow(length = unit(2.5, \"mm\")), # Make arrows smaller\n    show.legend = FALSE\n  ) +\n  geom_label(\n    data = first_obs, \n    show.legend = FALSE,\n    size = 2, # Make labels small\n    ### NB: alpha removed from here - labels will change transparency as well\n    ## as paths.\n  ) +\n  scale_x_reverse(expand = expansion(add = 0.7)) + # less expanasion needed.\n  scale_y_reverse() +\n  scale_alpha_manual(values = c('FALSE' = 0.2, 'TRUE' = 1)) +\n  facet_grid(\n    cols = vars(gender)\n  ) +\n  labs(\n    title = \"Vowel Space Change in NZE (yob: 1864-1981)\",\n    subtitle = \"Significant Shape Difference Between Genders\",\n    x = \"Second Formant (Lobanov 2.0)\",\n    y = \"First Formant (Lobanov 2.0)\"\n  )\n\nstatic_plot\n\n\n\n\nONZE smooths, with significant gender differences indicated.\n\n\n\n\nFigure (fig:onze-diff-plot) suggests that there are significant differences in the shape of change between the male and female speakers for nurse, goose, abd thought.\nThis can be converted into an animated plot using the same approach as for the animated plot in the previous section.\n\n\nPointwise Significance of Difference Smooths\nThe p-values in the summaries of difference smooth models provide a sense of whether the appropriate smooth for two levels of a factor differ in shape and overall. We might, instead, be interested in a pointwise, or even range-wise, sense of significance. In this case, we might ask if the smooths for men and women are significantly different for speakers born between 1860-1880 or between 1900-1920 etc. This is closely related to the visual method of significance testing which Sóskuthy expresses some caution about.\nThe method is simply to look at the plot of the smooths for each factor level in a difference smooth model and, if there is a point where their confidence intervals separate, to declare the difference significant. Sóskuthy advises that we only do this if we have a concrete hypothesis about the range in which the smooths will differ. If not, we open the door to an unacceptable number of false positives. (A second version of this approach looks at a plot of the difference between the two and looks for ranges in which 0 is not included within the confidence interval).\nThis caution is certainly important in a confirmatory setting. Here, in an exploratory setting, we can just be aware that we shouldn’t expend energy trying to interpret pointwise differences between two smooths. This is easy enough to avoid if we keep in mind our actual subject matter! If there are two years in our data in which men and women has a ‘significant’ difference between their smooths, say 1872 and 1873, we are not going to rush to the journals with a paper on the sociolinguistics of gender difference in 1872 and 1873. We just don’t expect that kind of resolution from this data and these methods of analysis.\nOne final note about this method is that it includes both the shape and the intercept terms. Two smooths which are identical in shape but are vertical translations of one another and do not have overlapping confidence intervals will appear as significantly different across their whole range.\nThe aim in this (final!) section will be to show how to extract ‘significantly different’ ranges from our smooths and to plot them in the vowel space. The core idea is to use the get_difference function from the itsadug package, which returns the difference smooth and its confidence intervals.\nLet’s look at this function for the fleece F1 model. First, it will be useful to see the difference smooth itself.\n\nplot_diff(\n  onze_models$model[[model_index]],\n  view = \"yob\",\n  comp = list(\"gender\" = c(\"F\", \"M\"))\n)\n\nSummary:\n    * yob : numeric predictor; with 100 values ranging from 1864.000000 to 1981.000000. \n    * speech_rate : numeric predictor; set to the value(s): 4.5576. \n    * speaker : factor; set to the value(s): IA_f_527. (Might be canceled as random effect, check below.) \n    * word : factor; set to the value(s): word_12921. (Might be canceled as random effect, check below.) \n    * NOTE : The following random effects columns are canceled: s(speaker),s(word)\n \n\n\n\n\n\n\n\n\n\n\nyob window(s) of significant difference(s):\n    1877.000000 - 1981.000000\n\n\nThe above plot shows the difference between female and male speakers in their smooths for fleece F1. The red line indicates the range of years of birth for which the two are taken to be significantly different.\nThe get_difference function helps us to find these ranges directly. For instance:\n\nto_predict &lt;- list(\n  \"yob\" = seq(from=1864, to=1981, by=1) # All years\n) \n\nfleece_difference_smooth &lt;- get_difference(\n  onze_models$model[[model_index]],\n  cond = to_predict,\n  comp = list(\"gender\" = c(\"F\", \"M\"))\n)\n\nSummary:\n    * yob : numeric predictor; with 118 values ranging from 1864.000000 to 1981.000000. \n    * speech_rate : numeric predictor; set to the value(s): 4.5576. \n    * speaker : factor; set to the value(s): IA_f_527. (Might be canceled as random effect, check below.) \n    * word : factor; set to the value(s): word_12921. (Might be canceled as random effect, check below.) \n    * NOTE : The following random effects columns are canceled: s(speaker),s(word)\n \n\nfleece_difference_smooth %&gt;%\n  mutate(\n    # If the estimate for the difference smooth minus the confidence interval\n    # value is greater than 0 or the difference smooth plus the confidence\n    # interval value is less than 0, then the confidence band does not include\n    # 0. In this case, the difference at that point is 'significant'.\n    sig_diff = (difference - CI) &gt; 0 | (difference + CI) &lt; 0\n  ) %&gt;%\n  head(15)\n\n    yob speech_rate  speaker       word difference        CI sig_diff\n1  1864      4.5576 IA_f_527 word_12921 0.05790359 0.1414914    FALSE\n2  1865      4.5576 IA_f_527 word_12921 0.06196859 0.1375988    FALSE\n3  1866      4.5576 IA_f_527 word_12921 0.06603054 0.1338369    FALSE\n4  1867      4.5576 IA_f_527 word_12921 0.07008477 0.1302179    FALSE\n5  1868      4.5576 IA_f_527 word_12921 0.07412524 0.1267523    FALSE\n6  1869      4.5576 IA_f_527 word_12921 0.07814509 0.1234480    FALSE\n7  1870      4.5576 IA_f_527 word_12921 0.08213722 0.1203095    FALSE\n8  1871      4.5576 IA_f_527 word_12921 0.08609447 0.1173372    FALSE\n9  1872      4.5576 IA_f_527 word_12921 0.09000985 0.1145281    FALSE\n10 1873      4.5576 IA_f_527 word_12921 0.09387715 0.1118768    FALSE\n11 1874      4.5576 IA_f_527 word_12921 0.09769055 0.1093757    FALSE\n12 1875      4.5576 IA_f_527 word_12921 0.10144559 0.1070174    FALSE\n13 1876      4.5576 IA_f_527 word_12921 0.10513947 0.1047953     TRUE\n14 1877      4.5576 IA_f_527 word_12921 0.10877103 0.1027050     TRUE\n15 1878      4.5576 IA_f_527 word_12921 0.11234046 0.1007440     TRUE\n\n\nThe above output gives the first 15 rows, and shows that the significant difference between smooths starts in 1876.\nTo apply the function to all models we use the following code:\n\nonze_models &lt;- onze_models %&gt;%\n  mutate(\n    sig_diffs = map(\n      model,\n      ~ get_difference(\n          .x,\n          cond = to_predict,\n          comp = list(\"gender\" = c(\"F\", \"M\")),\n          print.summary = FALSE\n        ) %&gt;%\n        mutate(\n          sig_diff = (difference - CI) &gt; 0 | (difference + CI) &lt; 0\n        ) %&gt;%\n        select(yob, sig_diff)\n    )\n  )\n\nWe want to take the years and whether the difference is significant for each model. We do this by unnesting in much the same way that we generated onze_predictions.\n\nonze_sig_diffs &lt;- onze_models %&gt;%\n  select(\n    vowel, formant_type, sig_diffs\n  ) %&gt;%\n  unnest(sig_diffs)\n\nWe then need to aggregate these ranges so that a difference appears as significant in the vowel space plot if either the F1 or the F2 smooths are significantly different across genders.\nThis is done as follows:\n\nonze_sig_diffs &lt;- onze_sig_diffs %&gt;%\n  pivot_wider(\n    names_from = formant_type,\n    values_from = sig_diff\n  ) %&gt;%\n  mutate(\n    sig_diff = F1_lob2 | F2_lob2\n  ) %&gt;%\n  select(\n    -(F1_lob2:F2_lob2)\n  )\n\nSince we already have predictions from these models from the previous section, I’ll add this information about significant ranges by using left_join.\n\nonze_predictions &lt;- onze_predictions %&gt;%\n  left_join(\n    onze_sig_diffs\n  )\n\nJoining with `by = join_by(vowel, yob)`\n\n\nFinally, we plot. In this case, we’ll go straight to the animated plot. The basic idea will be to have low transparency when differences significant and high transparency when they are not.\n\nonze_diff_anim &lt;- onze_predictions %&gt;%\n  ggplot(\n    aes(\n      x = fit_F2_lob2,\n      y = fit_F1_lob2,\n      colour = vowel,\n      fill = sig_diff,\n      label = vowel,\n      group = vowel,\n      alpha = sig_diff\n    )\n  ) +\n  geom_path(show.legend = FALSE) +\n  # NB: our labels just use the predictions dataframe now, so no need for the\n  # 'data = ' line.\n  geom_label(\n    show.legend = FALSE,\n    size = 2.5\n  ) +\n  scale_x_reverse(expand = expansion(add = 0.5)) + \n  scale_y_reverse() +\n  scale_alpha_manual(values = c('FALSE' = 0.2, 'TRUE' = 0.9)) +\n  scale_fill_manual(values = c(\"FALSE\" = \"grey\", \"TRUE\" = \"white\")) +\n  facet_grid(\n    cols = vars(gender)\n  ) +\n  labs(\n    title = \"Vowel Space Change in NZE (yob: 1864-1981)\",\n    subtitle = \"Significant Differences Between Gender Smooths\",\n    x = \"Second Formant (Lobanov 2.0)\",\n    y = \"First Formant (Lobanov 2.0)\",\n    caption = 'Year of Birth: {round(frame_along, 0)}\\n Grey indicates non-significant differences between male and\\n female smooths.' \n  ) +\n  theme(\n    plot.caption = element_text(size = 14, hjust = 0)\n  ) +\n  transition_reveal(along = yob)\n\nanimate(onze_diff_anim, end_pause = 10)\n\n`geom_path()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_path()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_path()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_path()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n\n\n\nIt remains to be seen how useful this kind of plot is for interpreting vowel space change. In this case, for many vowels we seem to start with insignificant difference, enter a long period of change, and, for many vowels, end up with insignificant differences. This would have to be distinguished from the fact that confidence intervals tend to flare out at the extremes of smooths."
  },
  {
    "objectID": "posts/significance-testing-for-vowel-space-gamms/index.html#conclusion",
    "href": "posts/significance-testing-for-vowel-space-gamms/index.html#conclusion",
    "title": "Model Check and Significance Testing for Vowel Space GAMMs",
    "section": "Conclusion",
    "text": "Conclusion\nThis post has expanded on my previous post on fitting independent models to the F1 and F2 data of multiple vowels using the nest and mutate approach of the purrr package. I’ve covered something of how to check the health of each of the models and how to extract information about whether the difference between two smooths is significant using some of the methods mentioned in Sóskuthy (2021). In particular, we have followed his advice about the use of difference smooths and the use of the Bonferroni correction."
  },
  {
    "objectID": "posts/new-website/index.html",
    "href": "posts/new-website/index.html",
    "title": "New Website with Quarto",
    "section": "",
    "text": "I have just been through the effort of switching my academic website over from blogdown — an interface between RMarkdown and Hugo Academic — to a Quarto website.\nA few aspects of my process might be useful to others (and to future me), so I’ve written them down here.\nAll code for the website is available here."
  },
  {
    "objectID": "posts/new-website/index.html#initial-set-up-and-theme",
    "href": "posts/new-website/index.html#initial-set-up-and-theme",
    "title": "New Website with Quarto",
    "section": "Initial Set Up and Theme",
    "text": "Initial Set Up and Theme\nI began with the standard ‘Creating a Website’ instructions, following the RStudio instructions.\nAs is clear from the documentation, there is a lot of moving back and forward between markdown files and the _quarto.yml file which sets overall options and structure for the website.\nI picked the litera theme from the provided options and created custom.scss to change the fonts. This is determined by the following, at the end of my _quarto.yml file.\nformat:\n  html:\n    theme: [litera, custom.scss]\n    css: styles.css\n    toc: true\nThe custom.scss file looks like this:\n/*-- scss:defaults --*/\n\n@import url('https://fonts.googleapis.com/css2?family=Goudy+Bookletter+1911&display=swap');\n@import url('https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&display=swap');\n\n$font-family-serif: \"EB Garamond\", Georgia, Cambria, \"Times New Roman\", Times, serif !default;\n$font-family-heading: \"Goudy Bookletter 1911\";\n$font-family-base: $font-family-serif;\n\n/*-- scss:rules --*/\n\nh1, h2, h3, h4, h5, h6, .menu-text, .navbar-title {\n  font-family: $font-family-heading;\n}\n\n.menu-text {\n  font-family: $font-family-heading;\n}\nI load the fonts from Google Fonts. To get a sense of what I wanted, I spent some time on the Fonts in Use page. I was feeling fancy, so I went with serifs for both the main text and heading text.\nIn the scss:defaults section I paste in the embed code from Google Fonts for EB Garamond (my base font) and Goudy Bookletter 1911 (my heading font). These are then assigned to variables. The $font-family-serif variable has a series of backups which I copied from another theme (I can’t recall which). The line $font-family-base: $font-family-serif is the one which switches all the body text to EB Garamond. The scss:rules section contains the code which changes the menu text and all headings to Goudy Bookletter 1911.\nA few other aesthetic points are worth mentioning. I added a favicon (this is the little image which appears in the corner of tabs on your browser). This image should be a small square (usually something like 32x32 pixels is fine, mine is a bit bigger). The image is how New Zealand appears in the Peirce quincuncial projection. This is in the images direction with the name favicon.png. This is then referenced in the _quarto.yml file: favicon: /images/favicon.png. Note that paths should start with a forward slash. Quarto will deal with the rest.\nI then put a small image in the footer.\nwebsite:\n  # I've skipped a few lines here.\n  page-footer:\n      center: |\n        [![](/images/logo.png)](https://paperspast.natlib.govt.nz/newspapers/AS19031024.2.47.11.6.1) \nThe image is a little mascot which I discovered in my research in the Papers Past collection. Again, the image is saved in the images folder. But this time the image is also a link to the original source using standard markdown syntax."
  },
  {
    "objectID": "posts/new-website/index.html#structure",
    "href": "posts/new-website/index.html#structure",
    "title": "New Website with Quarto",
    "section": "Structure",
    "text": "Structure\nThe website structure looks like this in _quarto.yml:\nwebsite:\n  # ...\n  navbar:\n    left:\n      - href: posts.qmd\n        text: Posts\n      - href: publications.qmd\n        text: Publications\n      - href: presentations.qmd\n        text: Presentations\n      - href: index.qmd\n        text: About\nThe order of these pages determines their order on the navigation bar of the website. The href entries give the name of the quarto document for the page and the text entry gives the text which will appear in the menu.\nThe bottom entry gives the ‘about’ page. The about page provides a short bio, employment, and educational history along with links to various other pages and profiles. I wanted the ‘about’ page to be the first page you arrive at when you go to my web address ( joshua.wilsonblack.nz). This is determined by the fact that it is called index.qmd. Whatever you have in index.qmd will be the home page of the website.\nThe first entry is a listing page for blog posts (posts.qmd). This has a very simple structure:\ntitle: \"Posts\"\nlisting:\n  contents: posts\n  categories: true\n  sort: \"date desc\"\n  type: default\n  feed: true\nThe line contents: posts means posts are read from a directory called posts. Each post has its own directory in the posts directory. For instance, this post is in the directory posts/new-website/. Within this directory, there is an index.qmd file with the post contents and any resources related to the post (images, datafiles, pdfs &c. &c.).\nI didn’t have many posts on my old website, so I transferred them across manually. The only thing I had to do was rename the markdown files index.qmd and change the yaml options to match those available in Quarto html documents.\nI have a presentations and publications page, which contain details of… my presentations and publications. Both generate a fairly straight forward bibliography, with my name highlighted when it appears (as is common in CVs).\nAnother line in _quarto.yml is significant for the structure of the website. I like to host html slides on my website. You can’t have quarto presentations as part of a quarto website (or, if you can it looks difficult). The resources option in\nproject:\n  type: website\n  resources: \n    - \"slides\"\nmeans that anything in the slides directory will be published. I can then generate slides elsewhere and copy across the .html file and any other resources into a directory in slides. For instance, my Australian Linguistic Society presentation from last year is accessible at https://joshua.wilsonblack.nz/slides/als-2023/, Here I make sure the .html file is named index.html."
  },
  {
    "objectID": "posts/new-website/index.html#wrangling",
    "href": "posts/new-website/index.html#wrangling",
    "title": "New Website with Quarto",
    "section": "Wrangling",
    "text": "Wrangling\nIt is a pain to have to write down the same information in many different places. This is not just an annoyance though: every time you re-enter information is an opportunity to introduce errors. If, instead, you have a ‘single source of truth’, you only have to get it right once.\nIn an ideal world, you would enter details of your employment, education, and research outputs in a single place and the information would propagate out to your website, your CV, your institutional profiles, and wherever else you might need it.\nORCID would be ideal for this. It has an API for reading information and can sync up with my University’s research maangement software (Symplectic Elements). In the current version of this website, I take my employment and education information from my ORCID profile. I’ve done this using the rorcid package. Have a look at the code in index.qmd. The basic idea is to download the information from my ORCID profile, wrangle it into a data frame, create a string in markdown format, and save this to an external file. I then load the file using the Quarto shortcode, e.g., {{&lt; include experience.md &gt;}}.\nI would like to do something similar for my publications and presentations. Perhaps I could download the information, save it to a bibtex file, and then generate a bibliography. I haven’t implemented this yet, but intend to for future publications and presentations.\nCurrently, I maintain a collection for my publications and my presentations in Zotero (along with the Better BibTeX plugin). These are exported as .bib files to the root directory of my website project.\nThe presentations.qmd file provides a good example of how this works:\ntitle: \"Presentations\"\nbibliography: presentations.bib\ncsl: chicago-mod.csl\nnocite: |\n  @*\nfilters: \n  - dociteproc\n  - boldname\n  - url2slides\nciteproc: false\nThe bibliography: presentations.bib file loads the bibliography. The line starting nocite: says that everything from the bibliography file should appear on the page. The rest is a little more tricky.\nI took the default bibliography style, which conforms to the Chicago Manual of Style, and slightly modified it. I took the .csl file from here, and renamed it chicago-mod.csl. My only change is in the &lt;sort&gt; section, as follows:\n&lt;sort&gt;\n  &lt;key macro=\"date\" sort=\"descending\"/&gt;\n  &lt;key macro=\"day-month\" sort=\"descending\"/&gt;\n  &lt;key variable=\"issued\"/&gt;\n  &lt;key variable=\"title\"/&gt;\n&lt;/sort&gt;\nThis means the items are sorted in reverse chronological order.\nThe final complication is the set of Lua filters, listed after filters and the option citeproc: false. The first two are explained here.\nThe url2slides filter takes any url on the presentations page and converts it to a hyper link with the text ‘[Slides]’. This looks a little neater to me. The key part of the Lua script looks like this:\nif el.content[k].t == \"Link\" then\n        local url = el.content[k].target\n        el.content[k] = pandoc.Link(\"[Slides]\", url)\nI know basically nothing about Lua. There are nice documentation pages from Quarto about Lua and filters."
  },
  {
    "objectID": "posts/new-website/index.html#publishing",
    "href": "posts/new-website/index.html#publishing",
    "title": "New Website with Quarto",
    "section": "Publishing",
    "text": "Publishing\nI followed the instructions to publish using Netlify via GitHub. This means the website will update the GitHub repository has a new commit. The one downside is that you have to include the rendered website (everything in the _site directory in the git repository — this can be a bit messy).\nOne benefit of hosting the website with Netlify is that it has straight forward redirection rules, which can are specified in the _redirects file. This is useful to ensure that links to blog posts and slides from my previous website will go to the corresponding pages on the new website. These rules can be as simple as, e.g., /post/* /posts/:splat. This means any address with /post will be directed to one with /posts. The posts folder on my old Hugo website was called post, but otherwise the urls are the same.\nFinally, I point the domain I registered using https://metaname.net (joshua.wilsonblack.nz) to the Netlify page."
  },
  {
    "objectID": "posts/contribution-to-pragmatic-reason/index.html",
    "href": "posts/contribution-to-pragmatic-reason/index.html",
    "title": "Peirce on Metaphysics and Common Sense Belief",
    "section": "",
    "text": "A volume in honour of Christopher Hookway is currently in the works edited by Robert Talisse, Paniel Reyes Cárdenas, and Daniel Herbert. I’ve contributed a piece on Chris’s account of Peirce’s metaphysics, especially as it appeals to ‘common sense’.\nOn Chris’s view, metaphysics articulates those features of common sense or instinct which are appealed to (often implicitly) in the generation of scientific hypotheses. Importantly, Chris takes it that the role of common sense diminishes as sciences develop. We might, then, expect the need for metaphysics to reduce over time (perhaps being replaced by… physics).\n\n\n\nChris and former students at The Idea of Pragmatism, a conference in his honour (I’m second from right).\n\n\nI argue that Chris’s emphasis on Peirce’s appeals to common sense belief hides Peirce’s more fundamental appeal to common experience. I emphasise Peirce’s notion of the ‘scientific intelligence’, or anything which can learn from experience (CP2.229, c. 1897). By abstraction, we make fallible claims about features of experience common to any scientific intelligence. This is the source of the strong modal claims which Peirce makes in his logic and metaphysics. I argue that common sense beliefs are only interesting for Peirce insofar as they give us access to common experience.\nDownload a pre-print of the paper here."
  },
  {
    "objectID": "posts/organising-r-projects/index.html",
    "href": "posts/organising-r-projects/index.html",
    "title": "Organising Shareable R Projects",
    "section": "",
    "text": "A complex research project can easily get out of hand organisationally. Sometimes we end up with multiple folders with different data sets, models, plots, scripts, and R Markdown files. This can become deeply confusing as we work. We want our projects to be well structured.\nWe also want our work to be shareable. Often newcomers to R produce files which depend on their own directory structure. A tell tale sign is a line of code like: my_data &lt;- read_rds('/home/jane_doe/Documents/my_project/data/third_attempt/data_2.rds). This will cause problems for anyone trying to run the code on a different computer.\nThe demand for well structured and shareable projects is doubly true if we want to make our data and analysis public by means of a GitHub or GitLab repository.\nHaving read a few blog posts and text book sections, I’ve ended up fixing on the following set up for my projects at NZILBB. This post assumes you are using RStudio."
  },
  {
    "objectID": "posts/organising-r-projects/index.html#the-problem",
    "href": "posts/organising-r-projects/index.html#the-problem",
    "title": "Organising Shareable R Projects",
    "section": "",
    "text": "A complex research project can easily get out of hand organisationally. Sometimes we end up with multiple folders with different data sets, models, plots, scripts, and R Markdown files. This can become deeply confusing as we work. We want our projects to be well structured.\nWe also want our work to be shareable. Often newcomers to R produce files which depend on their own directory structure. A tell tale sign is a line of code like: my_data &lt;- read_rds('/home/jane_doe/Documents/my_project/data/third_attempt/data_2.rds). This will cause problems for anyone trying to run the code on a different computer.\nThe demand for well structured and shareable projects is doubly true if we want to make our data and analysis public by means of a GitHub or GitLab repository.\nHaving read a few blog posts and text book sections, I’ve ended up fixing on the following set up for my projects at NZILBB. This post assumes you are using RStudio."
  },
  {
    "objectID": "posts/organising-r-projects/index.html#directory-structure",
    "href": "posts/organising-r-projects/index.html#directory-structure",
    "title": "Organising Shareable R Projects",
    "section": "Directory Structure",
    "text": "Directory Structure\nThe structure I recommend looks like this:\nproject_name\n| README.md\n| project_name.Rproj\n|\n└─── data\n|   | raw_data.csv\n|   | processed_data.rds\n|   | ...\n|\n└─── supplementaries\n|   | preprocessing.Rmd\n|   | modelling.Rmd\n|   | ...\n|\n└─── presentations\n|   └─── conference_name\n|       | presentation.Rmd\n|       | ...\n|\n└─── write_up\n|   | write_up.Rmd\n|   | ...\n|\n└─── scripts\n|   | anonymisation.R\n|   | ...\n|\n└─── models\n|   | model_name.rds\n|   | ...\n|\n└─── plots\n    | data_distribution.png\n    | model_predictions.png\n    | ...\n\nThe names of the files inside the folders above are illustrative examples.\nIn a file explorer, It’ll look like this: \nThe idea is to have a folder for each project. Within that folder, you will always have a data folder within which you should store all of your data files. Sometimes I distinguish between a raw data folder and one with data which has been processed as part of the project. This is not necessary though.\nThe majority of the R code which you will create will be in the supplementaries and scripts folders. I like to keep these separate, rather than having a single scripts folder for all .R and .Rmd files. This is because I tend to end up with lots of small scripts which respond to small projects which occur in the course of a project or which are prototypes of an analysis which will eventually appear in an R markdown document. The scripts folder can hold this, potentially messy, material. The supplementaries on the other hand, will be the main R markdown file containing the analysis code and any descriptions needed for someone to make sense of the steps you have carried out.\nThe R markdown files in the supplementaries folder will likely include much more than you report in any papers you produce from the project. In order to avoid unwieldy R markdown files which take a long time to knit, I like to have a distinct file for each major step in the project.\nWe often save plots in the course of our analysis. It is useful to have these in a plots folder. These can then be independently shared or used with other programmes.\nIn projects which handle large data sets, it is often wise to save any models we have fit to the data. These models can sometimes take hours to fit. This avoids wasting time refitting models. We put these files in the models folder.\nThe remaining folders in the structure above are optional. I often create presentations using the Xaringan or ioslides packages. I store these in a presentations directory. Sometimes I include a write_up folder for any journal articles or other written outputs which might come from the project.\nwithin the project folder, you should have a file called README.md. This is a markdown file which should explain what your project is, where things are stored within the project, and how the project can be interacted with. If you use GitHub or GitLab, the content of this file will be the first thing that any potential user will see of your project.\nWhat about the project_name.Rproj file? We’ll turn to that now."
  },
  {
    "objectID": "posts/organising-r-projects/index.html#r-project-files-and-the-here-package",
    "href": "posts/organising-r-projects/index.html#r-project-files-and-the-here-package",
    "title": "Organising Shareable R Projects",
    "section": "R Project Files and the here Package",
    "text": "R Project Files and the here Package\nThe main problem which R project files and the here package solve is file path management.\nThere are two options for starting a new project in RStudio: starting a project in an already existing directory or by creating a new directory. Both options will be given to you if you open RStudio and click the ‘project’ button at the top right of your screen and select ‘New Project…’. Either option is perfectly fine.\nOnce you have created a project you should see the name you have chosen at the top right of the screen. Your project folder, whether it already existed or you just created it, will also have a file with your project name and the extension .Rproj. This is the R project file. You can use this to open your project (just double click on it).\nOnce the project is opened, your working directory will be the project folder. This is already an improvement on getting and setting working directories with absolute file paths at the start of an R script. Anyone you send the project to will, if they open through the project file, be in the right place on their own machine to run your code. If you use relative file paths, this is a big improvement. That is, if you use e.g. read_csv('data/my_data.csv') from an R script in your project folder, this will work for anyone who you share your project with.\nThe here package takes us even further, though. As Jenny Richmond sets out, working directories work a little differently for R scripts and R markdown files. If you have a script and an Rmd file in project_name/markdown, the working directory for the R script will be the project folder, while the working directory for the R markdown file will be project_name/markdown. This means, for instance, that code which works in your markdown file will not work in the R console. It’s also something which you would have to keep in your head when moving between R scripts and R markdown files. This is not the kind of thing you want to be worrying about.\nIf you load the package here when working in a project, you can refer to files within your project in a consistent way which does not depend on anything specific to your own computer. For example:\nlibrary(here)\nmy_data &lt;- read_csv(here('data', 'my_data.csv'))\nThis code will load the package here, which looks ‘up’ the directory structure until it finds the R project file. We then use the function here to refer to files within our project. The arguments to here are either folders or files. A number of arguments specifying folders are followed by an argument with the name of the specific file to be loaded. In this case, 'data' is the first argument, which says we are looking in the data folder inside our project folder. The second argument, 'my_data.csv' says that we are looking for a file with that name inside the folder specified by the previous argument.\nTo belabour the point a bit: if the file we want to load is two folders deep, say, the file my_data.csv is inside a folder called data_source_1 which is in turn inside out data folder, then we would use here('data', 'data_source_1, my_data.csv). You can have as many arguments as you like when using the here function.\nA final note: in a footnote to the here package documentation it is suggested to avoid library(here) and instead use here::here() whenever you use the here function. I can see that this avoids clashes with the here function in the plyr package, but I don’t see this as likely to come up in any of my work or in NZILBB projects."
  },
  {
    "objectID": "posts/organising-r-projects/index.html#sharing-code",
    "href": "posts/organising-r-projects/index.html#sharing-code",
    "title": "Organising Shareable R Projects",
    "section": "Sharing Code",
    "text": "Sharing Code\nThere are two main code sharing scenarios:\n\nSharing a whole project.\nSharing a small portion of a project.\n\nIn the former case, the best option is Git, along with either GitHub or GitLab. This does have a bit of a learning curve (but it’s not insurmountable)!\nOtherwise, for either case, use whatever method you’d usually use to share files with someone. Either send the whole project folder, or the project folder without unnecessary files. Sometimes, it might be easier to start a new project and create a minimal working example of whatever it is you want to share."
  },
  {
    "objectID": "posts/organising-r-projects/index.html#futher-reading",
    "href": "posts/organising-r-projects/index.html#futher-reading",
    "title": "Organising Shareable R Projects",
    "section": "Futher reading",
    "text": "Futher reading\n\nBodo Winters, Statistics for Linguists: An Introduction Using R\n\nSee Chapter 2, which covers the core tidyverse packages with a worked example from linguistics and introduces a folder structure similar to the one suggested here in Section 2.8. Winters doesn’t cover the here package.\n\nHadley Wichkam, Advanced R, Chapter 8 Workflow: Projects.\n\nSome advice for setting up Rstudio to encourage good habits and reasons for using R projects to organise code.\n\nJenny Richmond, ‘how to use the here package`’\nMalcolm Barrett, ‘Why should I use the here package when I’m already using projects?’\n\nIf you want to know more about Git and Github for code sharing and version management check out Happy Git and GitHub for the useR"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joshua Wilson Black",
    "section": "",
    "text": "Kia ora, my name is Joshua Wilson Black. I am a lecturer at the New Zealand Institute of Language, Brain and Behaviour.\nMy research crosses linguistics, philosophy, and digital humanities. I am particularly interested in the use of large corpora to understand sign usage in many varieties and across a range of temporal scales using contempoary statistical methods. In all my work I try to live up to ‘open science’ principles.\nMy current research interests include:\nFrom 2024, I am teaching a series of workshops on R, statistics, and open science methods. These are targeted at advanced students and NZILBB-affiliated researchers, but are open to all. The materials for the workshops are available here and are constantly evolving. I’m very happy to receive any feedback on these materials.\nMy PhD was in philosophy and centred on the work of Charles Peirce and this remains a significant interest. My non-standard introduction to statistics came through trying to understand how statistical ideas functioned in Peirce’s metaphysics and philosophy of science. I have since updated my statistical and computational knowledge with a graduate degree in data science. Nonetheless, both the ‘what’ of my research — the quantitative investigation of sign use — and the ‘how’ — open science practices — remain Peircean in spirit."
  },
  {
    "objectID": "index.html#academic-employment",
    "href": "index.html#academic-employment",
    "title": "Joshua Wilson Black",
    "section": "Academic Employment",
    "text": "Academic Employment\n2024 - | Lecturer | University of Canterbury\n2021 - 2024 | Post Doctoral Fellow | University of Canterbury\n2014 - 2018 | Graduate Teaching Assistant | University of Sheffield"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Joshua Wilson Black",
    "section": "Education",
    "text": "Education\n2021 | Master of Applied Data Science (with Distinction) | University of Canterbury\n2017 | Doctor of Philosophy | University of Sheffield\n2013 | Master of Arts | University of Waikato\n2012 | Bachelor of Science with Honours (First Class) | University of Canterbury\n2011 | Bachelor of Science | University of Canterbury\n2011 | Bachelor of Arts | University of Canterbury"
  },
  {
    "objectID": "boldname/example.html",
    "href": "boldname/example.html",
    "title": "Boldname Example",
    "section": "",
    "text": "This filter adds formatting to heading text."
  },
  {
    "objectID": "boldname/example.html#heading",
    "href": "boldname/example.html#heading",
    "title": "Boldname Example",
    "section": "",
    "text": "This filter adds formatting to heading text."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "nzilbb.vowels 0.3.1\n\n\n\n\n\n\nlinguistics\n\n\nvowels\n\n\npca\n\n\n\n\n\n\n\n\n\nDec 1, 2024\n\n\nJoshua Wilson Black\n\n\n\n\n\n\n\n\n\n\n\n\nNew Website with Quarto\n\n\n\n\n\n\nquarto\n\n\n\n\n\n\n\n\n\nJun 20, 2024\n\n\nJoshua Wilson Black\n\n\n\n\n\n\n\n\n\n\n\n\nNew Publication: Creating Specialised Corpora from Digitized Historical Newspaper Archives\n\n\n\n\n\n\nhistory of philosophy\n\n\ndigital humanities\n\n\n\nNotice of new publication in Digital Scholarship in the Humanities. \n\n\n\n\n\nJan 26, 2023\n\n\nJoshua Wilson Black\n\n\n\n\n\n\n\n\n\n\n\n\nModel Check and Significance Testing for Vowel Space GAMMs\n\n\n\n\n\n\nGAMMs\n\n\nvowels\n\n\nsociolinguistics\n\n\n\nChecking GAMM models, significance tests, and visualisation for by-vowel models.\n\n\n\n\n\nAug 30, 2022\n\n\nJoshua Wilson Black\n\n\n\n\n\n\n\n\n\n\n\n\nOrganising Shareable R Projects\n\n\nSome advice for organising code for projects in linguistics using RStudio.\n\n\n\nlinguistics\n\n\nsociolinguistics\n\n\ndata science\n\n\n\nThere is plenty of advice out there for organising projects using R and RStudio. This post gathers together the advice I’ve found useful, especially with respect to sharing code and data between researchers. I run through directory stucture, the use of R project files, and the here package.\n\n\n\n\n\nAug 1, 2022\n\n\nJoshua Wilson Black\n\n\n\n\n\n\n\n\n\n\n\n\nA 3D PCA Visualisation with Plotly\n\n\n\n\n\n\ndata science\n\n\nvowels\n\n\npca\n\n\nplotly\n\n\n\nAttempts to produce an illustrative example of PCA using formant data and Plotly via R.\n\n\n\n\n\nJul 19, 2022\n\n\nJoshua Wilson Black\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising Vowel Space Change with GAMs\n\n\nA tidyverse Approach\n\n\n\nGAMMs\n\n\nvowels\n\n\nsociolinguistics\n\n\n\nA tutorial on visualising change in the vowel space of monopthongs using GAMs \n\n\n\n\n\nJun 29, 2022\n\n\nJoshua Wilson Black\n\n\n\n\n\n\n\n\n\n\n\n\nPeirce on Metaphysics and Common Sense Belief\n\n\n\n\n\n\nphilosophy\n\n\nPeirce\n\n\n\nA précis of my contribution to Pragmatic Reason.\n\n\n\n\n\nJun 28, 2022\n\n\nJoshua Wilson Black\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Wilson Black, Joshua, and Lynn Clark. 2024a. “PCA for Vocalic Covariation: Monophthongs and Diphthongs.” Presented at the Voices of Regional Australia workshop, Australian National University, November 21.\n\n\nSheard, Elena, and Joshua Wilson Black. 2024. “Rotating Principal Components to Explore Change over the Lifespan in New Zealand English Monophthongs.” Presented at the Eighteenth International Conference on Methods in Dialectology, La Trobe University, July 4. [Slides].\n\n\nSheard, Elena, Jen Hay, Robert Fromont, Joshua Wilson Black, and Lynn Clark. 2024. “Covarying New Zealand Vowels Interact with Speech Rate to Create Social Meaning for NZ Listeners.” Presented at the 19th Conference on Laboratory Phonology, Hanyang University, June 29.\n\n\nWilson Black, Joshua, and Lynn Clark. 2024b. “Exploring Vernacular Reorganisation in a Longitudinal Corpus of Pre-Schoolers’ Speech.” Presented at the 25th Sociolinguistics Symposium, Curtin University, June 24. [Slides].\n\n\nWilson Black, Joshua. 2023a. “Philosophy in a 19th Century New Zealand Newspaper Corpus.” Presented at the NZ Association of Philosophy Conferece, University of Waikato, Tauranga, December 6. [Slides].\n\n\n———. 2023b. “Sutherland and Popper: Lessons for Tauiwi Philosophers.” Presented at the mini-conference on Te Ao Māori and the Philosophy Curriculum in Aotearoa New Zealand, University of Waikato, Tauranga, December 3. [Slides].\n\n\nWilson Black, Joshua, and Lynn Clark. 2023a. “Early Aquisition of the New Zealand English Short Front Vowel Shift in a Longitudinal Corpus of Christchurch Pre-Schoolers.” Presented at the Annual Conference of the Linguistic Society of New Zealand, University of Auckland, November 30.\n\n\n———. 2023b. “Exploring Vernacular Reorganisation with Pre-Schoolers’ Short Front Vowels.” Presented at the Annual Conference of the Australian Linguistic Society, University of Sydney, November 30. [Slides].\n\n\nSheard, Elena, Robert Fromont, Joshua Wilson Black, Jen Hay, Lynn Clark, and Gia Hurring. 2023. “Exploring the Relationship Between the Production and Perception of Vocalic Covariation in New Zealand English.” Presented at the Annual Conference of the Linguistic Society of New Zealand, University of Auckland, November 29.\n\n\nSheard, Elena, and Joshua Wilson Black. 2023. “Change over the Lifespan Across Covarying New Zealand English Monophthongs.” Presented at the Annual Conference of the Australian Linguistic Society, University of Sydney, November.\n\n\nClark, Lynn, Joshua Wilson Black, James Brand, Gia Hurring, Jen Hay, Marton Sóskuthy, and Kevin Watson. 2023. “Understanding the Covariation of Vowel Variables Within and Across Speakers.” Presented at the Linguistics Invited Speaker Series, University of Pennsylvania, February 17.\n\n\nHay, Jennifer, and Joshua Wilson Black. 2022. “Principal Components Analysis in the Study of Language Variation — a Practical Workshop.” Presented at the Workshop of the Collaborative Research Cluster on Register, Leibniz-Zentrum Allgemeine Sprachwissenschaft, Berlin, December.\n\n\nWilson Black, Joshua, Jen Hay, Lynn Clark, and James Brand. 2022a. “Topical Structure, Amplitude Variation, and F1 in Single-Speaker Narrative Recordings.” Presented at the Annual Conference of the New Zealand Linguistic Society, University of Otago, December. [Slides].\n\n\nWilson Black, Joshua. 2022. “PCA in Sociolinguistics: A Tutorial.” Presented at the NZILBB Research Meeting, University of Canterbury, July. [Slides].\n\n\nFromont, Robert, Lynn Clark, Joshua Wilson Black, and Margaret Blackwood. 2022. “Forced Alignment of Child Speech: Comparing HTK and Kaldi-based Aligners.” Presented at the Third Workshop on Sociophonetic Variability in the English Varieties of Australia, Griffith University, May.\n\n\nWilson Black, Joshua, Jen Hay, Lynn Clark, and James Brand. 2022b. “The Overlooked Effect of Amplitude on Within-speaker Vowel Variation.” Presented at the Third Workshop on Sociophonetic Variability in the English Varieties of Australia, Griffith Universiry, May. [Slides].\n\n\nBlack, Joshua. 2021. “Philosophical Writing in Early New Zealand Newspapers: A Case Study of Corpus Construction from Large Digitised Newspaper Datasets.” Presented at the DHA2021 Ka Renarena Te Taukaea | Creating Communities, University of Canterbury, November 23.\n\n\nBlack, Joshua, Jen Hay, Lynn Clark, and Kevin Watson. 2021. “Quakebox Vowel Clustering: Corpus Level PCA.” Presented at the Sociolinguistics Seminar, University of Canterbury, September 10.\n\n\nBlack, Joshua. 2020a. “Peirce on God and Explanation.” Presented at the Philosophy Department Seminar, University of Canterbury, October 13.\n\n\n———. 2020b. “Philosophical Writing in Early New Zealand Newspapers: A Case Study of Corpus Construction from Large Digitised Newspaper Datasets.” Presented at the Sociolinguistics Seminar, University of Canterbury, April 19.\n\n\n———. 2018a. “Peirce on the Identity of Truth and Reality.” Presented at the Sixth World Congress and School on Universal Logic, Vichy, June 26.\n\n\n———. 2018b. “Peirce on God and Explanation.” Presented at the Christian Philosophy Conference, St John’s Seminary, Wonersh, January 11.\n\n\n———. 2017. “Peirce on Believing Propositions and Possessing Habits.” Presented at the British Society for the History of Philosophy Annual Conference 2017, University of Sheffield, April 7.\n\n\n———. 2016a. “Cenoscopy as First Philosophy.” Presented at the Way of Inquiry, University of Waikato, December 3.\n\n\n———. 2016b. “Brandom and Price on Expressivism and Representation.” Presented at the New Directions for Expressivism, University of Sheffield, August 19.\n\n\n———. 2016c. “Peirce on Exact Truth and Force Equivalence.” Presented at the Philosophy Department Graduate Seminar, University of Sheffield, June 1.\n\n\n———. 2016d. “The Normative Basis of Metaphysics and Pragmatist Naturalism.” Presented at the Naturalism and Normativity Workshop, University of the West of England, Bristol, May 13.\n\n\n———. 2015a. “Peirce’s Strong Anti-Intellectualism.” Presented at the University of Sheffield Summer Seminar, August 13.\n\n\n———. 2015b. “Quietism and Convergence: Motivating Pragmatist Metaphysics.” Presented at the Summer Institute in American Philosophy, University College Dublin, June 9.\n\n\n———. 2015c. “Quietism and Convergence: Motivating Pragmatist Metaphysics.” Presented at the White Rose Graduate Workshop, University of Hull, May 23.\n\n\n———. 2014a. “Peirce and Sellars on the Problem of Universals.” Presented at the New Zealand Association of Philosophy Conference, University of Canterbury, December 4.\n\n\n———. 2014b. “Hunting for Metaphysics in Price’s Pragmatism.” Presented at the University of Sheffield Graduate Seminar, University of Sheffield, October 15.\n\n\n———. 2014c. “Peirce on Habit and the Theory/Practice Distinction.” Presented at the Charles S. Peirce international centennial congress, UMass Lowell, July 17.\n\n\n———. 2014d. “Peirce and Price on Pragmatist Metaphysics.” Presented at the Summer Institute in American Philosophy, University of Oregon, July 10.\n\n\n———. 2014e. “Habit and Peirce’s Theory/Practice Distinction.” Presented at the University of Sheffield Graduate Seminar, University of Sheffield, February 12.\n\n\n———. 2013. “Peirce on the Law of Habit and the Law of Large Numbers.” Presented at the Graduate Idealism and Pragmatism Workshop, University of Sheffield, October 14.\n\n\n———. 2012. “Peirce on Habit and Self-Control.” Presented at the New Zealand AAP Conference, Victoria University of Wellington, December 6.\n\n\n———. 2011. “Formalism and Lakatos’s Philosophy of Mathematics.” Presented at the University of Canterbury Philosophy Retreat, Hokitika, New Zealand."
  }
]